{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Neural Network, CUDA, and PTX – Forward Propagation\n",
    "\n",
    "In this project we implement a simple neural network with one hidden layer. The network architecture is as follows:\n",
    "- **Input Layer:** Receives a flattened MNIST image of dimension $784$ (i.e. $28 \\times 28$ pixels).\n",
    "- **Hidden Layer:** Consists of $128$ neurons with ReLU activation.\n",
    "- **Output Layer:** Contains $10$ neurons (one per digit class) with a linear activation.\n",
    "\n",
    "**CUDA and PTX Overview:**  \n",
    "CUDA is NVIDIA’s parallel computing platform and API that allows developers to run massively parallel computations on GPUs. CUDA code is compiled into PTX (Parallel Thread Execution), which is a low-level, assembly-like language for NVIDIA GPUs. PTX is then just-in-time (JIT) compiled by the GPU driver into device-specific binary instructions. This low-level control can be leveraged to optimize performance-critical parts of an algorithm.\n",
    "\n",
    "**Forward Propagation in PTX:**  \n",
    "In the PTX (or higher-level CUDA C) implementation, forward propagation is performed as follows:\n",
    "1. **Layer 1 Computation:**  \n",
    "   Each neuron in the hidden layer computes  \n",
    "   $$ a^{(1)} = \\max\\Big(0, \\sum_{j=1}^{784} W^{(1)}_{ij} \\, x_j + b^{(1)}_i \\Big) $$\n",
    "   where each thread processes one sample and loops over the input elements, performing multiplication, summation, and finally applying the ReLU activation ($\\max(0,\\cdot)$).\n",
    "2. **Output Layer Computation:**  \n",
    "   The output layer computes  \n",
    "   $$ a^{(2)} = \\sum_{j=1}^{128} W^{(2)}_{ij} \\, a^{(1)}_j + b^{(2)}_i $$\n",
    "   again using loops to perform the dot products.\n",
    "\n",
    "In PTX, these computations are implemented by carefully managing pointer arithmetic, register usage, and ensuring that each thread handles one sample from the mini-batch.\n",
    "\n",
    "---\n",
    "\n",
    "# 2. Backpropagation Computation in PTX\n",
    "\n",
    "Backpropagation for our network is designed to compute gradients for both layers using a squared-error loss with one-hot targets. The algorithm is as follows:\n",
    "\n",
    "1. **Output Layer Backpropagation:**  \n",
    "   The error (delta) at the output is computed by  \n",
    "   $$ \\delta^{(2)} = a^{(2)} - y $$\n",
    "   where $y$ is the one-hot target vector. The gradients for the output biases are then given by $\\delta^{(2)}$, and the gradient for the output weights is computed as  \n",
    "   $$ \\nabla W^{(2)} = \\delta^{(2)} \\otimes a^{(1)} $$\n",
    "2. **Hidden Layer Backpropagation:**  \n",
    "   The error for the hidden layer is computed by backpropagating the output error through the weights and applying the derivative of the ReLU activation:\n",
    "   $$ \\delta^{(1)} = \\Big( (W^{(2)})^T \\, \\delta^{(2)} \\Big) \\odot \\mathbf{1}_{\\{a^{(1)} > 0\\}} $$\n",
    "   where $\\mathbf{1}_{\\{a^{(1)} > 0\\}}$ is the indicator function representing the derivative of the ReLU (1 if $a^{(1)}>0$, else 0). The gradients for the input-to-hidden weights and biases are then computed as  \n",
    "   $$ \\nabla W^{(1)} = \\delta^{(1)} \\otimes x. $$\n",
    "   \n",
    "**PTX Implementation Details:**  \n",
    "The PTX (or corresponding CUDA C) implementation translates these steps into a series of loops with explicit memory loads and stores. Temporary arrays are allocated (often in registers or local memory) to hold the intermediate delta values. However, due to limitations such as the inability to index register arrays easily and strict operand type matching requirements, the PTX implementation of backpropagation encountered several issues and did not work as expected without further debugging and modifications.\n",
    "\n",
    "---\n",
    "\n",
    "# 3. Training Setup Implementation in CUDA\n",
    "\n",
    "The training is implemented in two main CUDA kernels:\n",
    "\n",
    "1. **trainStep Kernel:**  \n",
    "   - **Input:** A mini-batch of images and one-hot encoded labels.\n",
    "   - **Forward Pass:** For each sample, the kernel computes the hidden layer activations using $a^{(1)} = \\max(0, W^{(1)} x + b^{(1)})$, and then computes the output $a^{(2)} = W^{(2)} a^{(1)} + b^{(2)}$.\n",
    "   - **Backward Pass:** The kernel computes the output error $\\delta^{(2)} = a^{(2)} - y$ and backpropagates this error to compute $\\delta^{(1)}$ for the hidden layer. Gradients are calculated as outer products of the error terms with the corresponding activations.\n",
    "   - **Output:** The gradients for each sample are written into per-sample gradient buffers.\n",
    "\n",
    "2. **updateWeights Kernel:**  \n",
    "   - **Input:** The gradient buffers and the current parameter values.\n",
    "   - **Computation:** This kernel aggregates the per-sample gradients by summing them over the mini-batch, averages them (dividing by the batch size), and updates the parameters using the rule  \n",
    "     $$ W \\leftarrow W - \\gamma \\cdot \\frac{1}{N}\\sum_{i=1}^{N} \\nabla W_i $$\n",
    "     where $\\gamma$ is the learning rate and $N$ is the batch size.\n",
    "\n",
    "**Overall Training Loop:**  \n",
    "- The host code sets up GPU memory for inputs, activations, parameters, and gradient buffers.\n",
    "- For each mini-batch, the data is transferred to the GPU.\n",
    "- The `trainStep` kernel is launched to perform forward propagation and compute gradients.\n",
    "- The `updateWeights` kernel is then invoked for each parameter to update the network.\n",
    "- Periodically, the parameters are copied back to the host and the network’s performance is evaluated using a NumPy forward pass.\n",
    "- This process continues until the test accuracy exceeds 60% or a maximum number of batches is processed.\n",
    "\n",
    "---\n",
    "\n",
    "# 4. Comparison with PyTorch\n",
    "\n",
    "To evaluate our custom CUDA implementation, we compared it with a PyTorch implementation that uses an identical two-layer network architecture and batch gradient updates with mean squared error (MSE) loss on one-hot targets.\n",
    "\n",
    "**PyCUDA Results:**  \n",
    "The PyCUDA implementation (a two-layer network with one hidden layer of 128 neurons) was trained on the MNIST dataset with a batch size of $64$. Training proceeded until the test accuracy exceeded 60%. Here are some of the recorded test accuracies:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "forward_pass = r\"\"\"\n",
    ".version 7.0\n",
    ".target sm_35\n",
    ".address_size 64\n",
    "\n",
    "// -------------------------------------------------------------------\n",
    "// Kernel: neuralNetBatch\n",
    "//\n",
    "// This kernel computes a forward pass for a 3-layer feed-forward neural\n",
    "// network over a batch of inputs. Each thread processes one sample.\n",
    "// For each sample, the network performs:\n",
    "//\n",
    "//   Layer 1 (Input -> Hidden1):\n",
    "//     z^(1) = W^(1)*x + b^(1)\n",
    "//     a^(1) = ReLU(z^(1))\n",
    "//\n",
    "//   Layer 2 (Hidden1 -> Hidden2):\n",
    "//     z^(2) = W^(2)*a^(1) + b^(2)\n",
    "//     a^(2) = ReLU(z^(2))\n",
    "//\n",
    "//   Layer 3 (Hidden2 -> Output):\n",
    "//     z^(3) = W^(3)*a^(2) + b^(3)\n",
    "//     (The output here is raw logits; softmax may be applied later.)\n",
    "//\n",
    "// The weight matrices and biases are shared by all threads while the input,\n",
    "// intermediate activations, and outputs are stored in batched arrays.\n",
    "// -------------------------------------------------------------------\n",
    ".visible .entry neuralNetBatch(\n",
    "    .param .u64 input_ptr,       // [batch_size x input_dim] array\n",
    "    .param .u64 weights1_ptr,    // Layer 1 weights: [hidden1_dim x input_dim]\n",
    "    .param .u64 bias1_ptr,       // Layer 1 biases: [hidden1_dim]\n",
    "    .param .u64 hidden1_ptr,     // Layer 1 activations: [batch_size x hidden1_dim]\n",
    "    .param .u64 weights2_ptr,    // Layer 2 weights: [hidden2_dim x hidden1_dim]\n",
    "    .param .u64 bias2_ptr,       // Layer 2 biases: [hidden2_dim]\n",
    "    .param .u64 hidden2_ptr,     // Layer 2 activations: [batch_size x hidden2_dim]\n",
    "    .param .u64 weights3_ptr,    // Layer 3 weights: [output_dim x hidden2_dim]\n",
    "    .param .u64 bias3_ptr,       // Layer 3 biases: [output_dim]\n",
    "    .param .u64 output_ptr,      // Output logits: [batch_size x output_dim]\n",
    "    .param .u32 batch_size,      // Total number of samples in the batch\n",
    "    .param .u32 input_dim,       // Dimensionality of input vector\n",
    "    .param .u32 hidden1_dim,     // Number of neurons in Layer 1\n",
    "    .param .u32 hidden2_dim,     // Number of neurons in Layer 2\n",
    "    .param .u32 output_dim       // Number of output neurons\n",
    ")\n",
    "{\n",
    "    // ---------------------------------------------------------------\n",
    "    // Declare registers.\n",
    "    // Note: We rename the loaded batch_size to \"bs\" to avoid duplicate names.\n",
    "    // ---------------------------------------------------------------\n",
    "    .reg .u64   in_base, w1_base, b1_base, h1_base, w2_base, b2_base, h2_base, w3_base, b3_base, out_base;\n",
    "    .reg .u32   bs, inputDim, hidden1Dim, hidden2Dim, outputDim;\n",
    "    .reg .u32   tid, blockId, blockDim, sampleIdx;\n",
    "    .reg .u32   i, j, idx, idx_byte;\n",
    "    .reg .u64   byte_offset;\n",
    "    .reg .u64   addr_w, addr_bias;\n",
    "    .reg .f32   acc, temp, in_val, wt, bias;\n",
    "    .reg .u64   sample_in_ptr, sample_h1_ptr, sample_h2_ptr, sample_out_ptr;\n",
    "    .reg .pred  p_exit;\n",
    "\n",
    "    // ---------------------------------------------------------------\n",
    "    // Load kernel parameters.\n",
    "    // ---------------------------------------------------------------\n",
    "    ld.param.u64   in_base,   [input_ptr];\n",
    "    ld.param.u64   w1_base,   [weights1_ptr];\n",
    "    ld.param.u64   b1_base,   [bias1_ptr];\n",
    "    ld.param.u64   h1_base,   [hidden1_ptr];\n",
    "    ld.param.u64   w2_base,   [weights2_ptr];\n",
    "    ld.param.u64   b2_base,   [bias2_ptr];\n",
    "    ld.param.u64   h2_base,   [hidden2_ptr];\n",
    "    ld.param.u64   w3_base,   [weights3_ptr];\n",
    "    ld.param.u64   b3_base,   [bias3_ptr];\n",
    "    ld.param.u64   out_base,  [output_ptr];\n",
    "    ld.param.u32   bs, [batch_size];       // Load batch size into \"bs\"\n",
    "    ld.param.u32   inputDim,  [input_dim];\n",
    "    ld.param.u32   hidden1Dim,[hidden1_dim];\n",
    "    ld.param.u32   hidden2Dim,[hidden2_dim];\n",
    "    ld.param.u32   outputDim, [output_dim];\n",
    "\n",
    "    // ---------------------------------------------------------------\n",
    "    // Compute global thread index (sampleIdx) using built-in registers.\n",
    "    // sampleIdx = blockIdx.x * blockDim.x + threadIdx.x\n",
    "    // ---------------------------------------------------------------\n",
    "    mov.u32   tid, %tid.x;\n",
    "    mov.u32   blockId, %ctaid.x;\n",
    "    mov.u32   blockDim, %ntid.x;\n",
    "    mul.lo.u32 sampleIdx, blockId, blockDim;\n",
    "    add.u32    sampleIdx, sampleIdx, tid;\n",
    "\n",
    "    // If sampleIdx >= bs then exit this thread.\n",
    "    setp.ge.u32 p_exit, sampleIdx, bs;\n",
    "    @p_exit bra exit;\n",
    "\n",
    "    // ---------------------------------------------------------------\n",
    "    // Compute sample-specific base pointers (each sample stored contiguously):\n",
    "    //   sample_in_ptr = in_base + sampleIdx * inputDim * 4\n",
    "    //   sample_h1_ptr = h1_base + sampleIdx * hidden1Dim * 4\n",
    "    //   sample_h2_ptr = h2_base + sampleIdx * hidden2Dim * 4\n",
    "    //   sample_out_ptr = out_base + sampleIdx * outputDim * 4\n",
    "    // ---------------------------------------------------------------\n",
    "    mul.lo.u32 idx, sampleIdx, inputDim;\n",
    "    mul.lo.u32 idx_byte, idx, 4;\n",
    "    cvt.u64.u32 byte_offset, idx_byte;\n",
    "    add.u64 sample_in_ptr, in_base, byte_offset;\n",
    "\n",
    "    mul.lo.u32 idx, sampleIdx, hidden1Dim;\n",
    "    mul.lo.u32 idx_byte, idx, 4;\n",
    "    cvt.u64.u32 byte_offset, idx_byte;\n",
    "    add.u64 sample_h1_ptr, h1_base, byte_offset;\n",
    "\n",
    "    mul.lo.u32 idx, sampleIdx, hidden2Dim;\n",
    "    mul.lo.u32 idx_byte, idx, 4;\n",
    "    cvt.u64.u32 byte_offset, idx_byte;\n",
    "    add.u64 sample_h2_ptr, h2_base, byte_offset;\n",
    "\n",
    "    mul.lo.u32 idx, sampleIdx, outputDim;\n",
    "    mul.lo.u32 idx_byte, idx, 4;\n",
    "    cvt.u64.u32 byte_offset, idx_byte;\n",
    "    add.u64 sample_out_ptr, out_base, byte_offset;\n",
    "\n",
    "    // ===============================================================\n",
    "    // Layer 1: Compute hidden1 = ReLU(W1 * input + b1)\n",
    "    // ===============================================================\n",
    "    mov.u32 j, 0;\n",
    "layer1_loop:\n",
    "    setp.ge.u32 p_exit, j, hidden1Dim;\n",
    "    @p_exit bra layer1_end;\n",
    "\n",
    "    // Initialize accumulator for neuron j to 0.\n",
    "    mov.f32 acc, 0f00000000;\n",
    "    mov.u32 i, 0;\n",
    "layer1_inner_loop:\n",
    "    setp.ge.u32 p_exit, i, inputDim;\n",
    "    @p_exit bra layer1_after_inner;\n",
    "\n",
    "    // Compute weight index: idx = j * inputDim + i.\n",
    "    mul.lo.u32 idx, j, inputDim;\n",
    "    add.u32 idx, idx, i;\n",
    "    mul.lo.u32 idx_byte, idx, 4;\n",
    "    cvt.u64.u32 byte_offset, idx_byte;\n",
    "    add.u64 addr_w, w1_base, byte_offset;\n",
    "    ld.global.f32 wt, [addr_w];\n",
    "\n",
    "    // Load input: sample_in_ptr + i*4.\n",
    "    mul.lo.u32 idx_byte, i, 4;\n",
    "    cvt.u64.u32 byte_offset, idx_byte;\n",
    "    add.u64 addr_w, sample_in_ptr, byte_offset;\n",
    "    ld.global.f32 in_val, [addr_w];\n",
    "\n",
    "    // Multiply and accumulate.\n",
    "    mul.f32 temp, wt, in_val;\n",
    "    add.f32 acc, acc, temp;\n",
    "    add.u32 i, i, 1;\n",
    "    bra layer1_inner_loop;\n",
    "layer1_after_inner:\n",
    "    // Load bias for neuron j.\n",
    "    mul.lo.u32 idx_byte, j, 4;\n",
    "    cvt.u64.u32 byte_offset, idx_byte;\n",
    "    add.u64 addr_bias, b1_base, byte_offset;\n",
    "    ld.global.f32 bias, [addr_bias];\n",
    "    add.f32 acc, acc, bias;\n",
    "    // Apply ReLU: acc = max(0, acc)\n",
    "    max.f32 acc, acc, 0f00000000;\n",
    "    // Store activation in hidden1.\n",
    "    mul.lo.u32 idx_byte, j, 4;\n",
    "    cvt.u64.u32 byte_offset, idx_byte;\n",
    "    add.u64 addr_w, sample_h1_ptr, byte_offset;\n",
    "    st.global.f32 [addr_w], acc;\n",
    "    add.u32 j, j, 1;\n",
    "    bra layer1_loop;\n",
    "layer1_end:\n",
    "\n",
    "    // ===============================================================\n",
    "    // Layer 2: Compute hidden2 = ReLU(W2 * hidden1 + b2)\n",
    "    // ===============================================================\n",
    "    mov.u32 j, 0;\n",
    "layer2_loop:\n",
    "    setp.ge.u32 p_exit, j, hidden2Dim;\n",
    "    @p_exit bra layer2_end;\n",
    "    mov.f32 acc, 0f00000000;\n",
    "    mov.u32 i, 0;\n",
    "layer2_inner_loop:\n",
    "    setp.ge.u32 p_exit, i, hidden1Dim;\n",
    "    @p_exit bra layer2_after_inner;\n",
    "    mul.lo.u32 idx, j, hidden1Dim;\n",
    "    add.u32 idx, idx, i;\n",
    "    mul.lo.u32 idx_byte, idx, 4;\n",
    "    cvt.u64.u32 byte_offset, idx_byte;\n",
    "    add.u64 addr_w, w2_base, byte_offset;\n",
    "    ld.global.f32 wt, [addr_w];\n",
    "    // Load activation from hidden1.\n",
    "    mul.lo.u32 idx_byte, i, 4;\n",
    "    cvt.u64.u32 byte_offset, idx_byte;\n",
    "    add.u64 addr_w, sample_h1_ptr, byte_offset;\n",
    "    ld.global.f32 in_val, [addr_w];\n",
    "    mul.f32 temp, wt, in_val;\n",
    "    add.f32 acc, acc, temp;\n",
    "    add.u32 i, i, 1;\n",
    "    bra layer2_inner_loop;\n",
    "layer2_after_inner:\n",
    "    // Add bias for layer 2 neuron j.\n",
    "    mul.lo.u32 idx_byte, j, 4;\n",
    "    cvt.u64.u32 byte_offset, idx_byte;\n",
    "    add.u64 addr_bias, b2_base, byte_offset;\n",
    "    ld.global.f32 bias, [addr_bias];\n",
    "    add.f32 acc, acc, bias;\n",
    "    max.f32 acc, acc, 0f00000000;\n",
    "    // Store activation in hidden2.\n",
    "    mul.lo.u32 idx_byte, j, 4;\n",
    "    cvt.u64.u32 byte_offset, idx_byte;\n",
    "    add.u64 addr_w, sample_h2_ptr, byte_offset;\n",
    "    st.global.f32 [addr_w], acc;\n",
    "    add.u32 j, j, 1;\n",
    "    bra layer2_loop;\n",
    "layer2_end:\n",
    "\n",
    "    // ===============================================================\n",
    "    // Layer 3: Compute output = W3 * hidden2 + b3\n",
    "    // ===============================================================\n",
    "    mov.u32 j, 0;\n",
    "layer3_loop:\n",
    "    setp.ge.u32 p_exit, j, outputDim;\n",
    "    @p_exit bra layer3_end;\n",
    "    mov.f32 acc, 0f00000000;\n",
    "    mov.u32 i, 0;\n",
    "layer3_inner_loop:\n",
    "    setp.ge.u32 p_exit, i, hidden2Dim;\n",
    "    @p_exit bra layer3_after_inner;\n",
    "    mul.lo.u32 idx, j, hidden2Dim;\n",
    "    add.u32 idx, idx, i;\n",
    "    mul.lo.u32 idx_byte, idx, 4;\n",
    "    cvt.u64.u32 byte_offset, idx_byte;\n",
    "    add.u64 addr_w, w3_base, byte_offset;\n",
    "    ld.global.f32 wt, [addr_w];\n",
    "    // Load activation from hidden2.\n",
    "    mul.lo.u32 idx_byte, i, 4;\n",
    "    cvt.u64.u32 byte_offset, idx_byte;\n",
    "    add.u64 addr_w, sample_h2_ptr, byte_offset;\n",
    "    ld.global.f32 in_val, [addr_w];\n",
    "    mul.f32 temp, wt, in_val;\n",
    "    add.f32 acc, acc, temp;\n",
    "    add.u32 i, i, 1;\n",
    "    bra layer3_inner_loop;\n",
    "layer3_after_inner:\n",
    "    // Add bias for output neuron j.\n",
    "    mul.lo.u32 idx_byte, j, 4;\n",
    "    cvt.u64.u32 byte_offset, idx_byte;\n",
    "    add.u64 addr_bias, b3_base, byte_offset;\n",
    "    ld.global.f32 bias, [addr_bias];\n",
    "    add.f32 acc, acc, bias;\n",
    "    // Store the output logit.\n",
    "    mul.lo.u32 idx_byte, j, 4;\n",
    "    cvt.u64.u32 byte_offset, idx_byte;\n",
    "    add.u64 addr_w, sample_out_ptr, byte_offset;\n",
    "    st.global.f32 [addr_w], acc;\n",
    "    add.u32 j, j, 1;\n",
    "    bra layer3_loop;\n",
    "layer3_end:\n",
    "exit:\n",
    "    ret;\n",
    "}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have the python code to run the above PTX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden layer 1 activations shape: (10, 128)\n",
      "Hidden layer 2 activations shape: (10, 64)\n",
      "Output layer (logits) shape: (10, 10)\n",
      "Output logits for each sample:\n",
      "[[-2398.4146     896.48694  -2454.5962    -310.93024    364.0878\n",
      "    316.3618     952.67584  -1174.3683    -400.76767   -694.3439  ]\n",
      " [ -218.6154    1192.3281    -988.25446    918.27496  -1215.4276\n",
      "    191.75932    537.2199    1248.4154     -92.95179    184.19208 ]\n",
      " [-2518.4102    1683.3507   -1737.7667    1169.7318    -601.6049\n",
      "    210.67848   1875.9261    -862.35266   -549.42114    860.8875  ]\n",
      " [    6.430615  3396.2239    -629.7285    2261.9907   -1670.4064\n",
      "   -264.03015   -484.3882     124.95582   -415.25583   -449.62613 ]\n",
      " [  371.7862   -1066.0829   -1795.1741    2092.619     -522.11346\n",
      "   -756.5303     382.81982   1786.3528    -273.72974    546.8591  ]\n",
      " [ -346.25864   -179.3902    -174.4288    2052.9424   -1083.8575\n",
      "    796.1063     366.92545   1549.8699    -730.0691    1120.9371  ]\n",
      " [  510.4109     337.61234    442.06943   -768.5236   -1563.8579\n",
      "    432.71738    -35.607933  -964.6857     670.0221    2535.4143  ]\n",
      " [  985.91125   1050.3823    -940.3928    2348.7686    -801.338\n",
      "   -575.2336   -1105.0238     132.9021    -468.47363  -2258.5725  ]\n",
      " [  -58.609905   961.8627    -648.20496   3182.0784   -1847.2542\n",
      "   -620.86865    737.44354    439.33105  -1742.1221   -1452.2421  ]\n",
      " [  483.08945   -844.0487   -2989.3762    3406.7214     665.5422\n",
      "  -2480.4941    1350.8539     -23.282099  -249.72057   1847.2515  ]]\n"
     ]
    }
   ],
   "source": [
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "import numpy as np\n",
    "\n",
    "# -------------------------------\n",
    "# Define network dimensions:\n",
    "#   For MNIST: input_dim = 784.\n",
    "#   Here we choose arbitrary hidden layer sizes.\n",
    "# -------------------------------\n",
    "batch_size   = 10   # Number of samples in the batch.\n",
    "input_dim    = 784\n",
    "hidden1_dim  = 128\n",
    "hidden2_dim  = 64\n",
    "output_dim   = 10\n",
    "\n",
    "# -------------------------------\n",
    "# Create random data for testing:\n",
    "#   Each sample is a row in the input array.\n",
    "#   We initialize weight matrices and biases with random data.\n",
    "# -------------------------------\n",
    "input_host   = np.random.randn(batch_size, input_dim).astype(np.float32)\n",
    "W1_host      = np.random.randn(hidden1_dim, input_dim).astype(np.float32)\n",
    "b1_host      = np.random.randn(hidden1_dim).astype(np.float32)\n",
    "W2_host      = np.random.randn(hidden2_dim, hidden1_dim).astype(np.float32)\n",
    "b2_host      = np.random.randn(hidden2_dim).astype(np.float32)\n",
    "W3_host      = np.random.randn(output_dim, hidden2_dim).astype(np.float32)\n",
    "b3_host      = np.random.randn(output_dim).astype(np.float32)\n",
    "\n",
    "# Output buffers for intermediate activations and final logits.\n",
    "hidden1_host = np.empty((batch_size, hidden1_dim), dtype=np.float32)\n",
    "hidden2_host = np.empty((batch_size, hidden2_dim), dtype=np.float32)\n",
    "output_host  = np.empty((batch_size, output_dim), dtype=np.float32)\n",
    "\n",
    "# -------------------------------\n",
    "# Allocate device memory and copy host data:\n",
    "# -------------------------------\n",
    "input_gpu   = cuda.mem_alloc(input_host.nbytes)\n",
    "W1_gpu      = cuda.mem_alloc(W1_host.nbytes)\n",
    "b1_gpu      = cuda.mem_alloc(b1_host.nbytes)\n",
    "hidden1_gpu = cuda.mem_alloc(hidden1_host.nbytes)\n",
    "W2_gpu      = cuda.mem_alloc(W2_host.nbytes)\n",
    "b2_gpu      = cuda.mem_alloc(b2_host.nbytes)\n",
    "hidden2_gpu = cuda.mem_alloc(hidden2_host.nbytes)\n",
    "W3_gpu      = cuda.mem_alloc(W3_host.nbytes)\n",
    "b3_gpu      = cuda.mem_alloc(b3_host.nbytes)\n",
    "output_gpu  = cuda.mem_alloc(output_host.nbytes)\n",
    "\n",
    "cuda.memcpy_htod(input_gpu, input_host)\n",
    "cuda.memcpy_htod(W1_gpu, W1_host)\n",
    "cuda.memcpy_htod(b1_gpu, b1_host)\n",
    "cuda.memcpy_htod(W2_gpu, W2_host)\n",
    "cuda.memcpy_htod(b2_gpu, b2_host)\n",
    "cuda.memcpy_htod(W3_gpu, W3_host)\n",
    "cuda.memcpy_htod(b3_gpu, b3_host)\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Load the PTX module and get the kernel function.\n",
    "# -------------------------------\n",
    "module = cuda.module_from_buffer(forward_pass.encode(\"utf-8\"))\n",
    "kernel = module.get_function(\"neuralNetBatch\")\n",
    "\n",
    "# -------------------------------\n",
    "# Configure grid and block dimensions.\n",
    "#\n",
    "# Since we want one thread per sample, we set the total number of threads\n",
    "# equal to batch_size.\n",
    "# -------------------------------\n",
    "threads_per_block = batch_size if batch_size <= 1024 else 1024\n",
    "blocks = (batch_size + threads_per_block - 1) // threads_per_block\n",
    "\n",
    "# -------------------------------\n",
    "# Launch the kernel.\n",
    "# -------------------------------\n",
    "kernel(\n",
    "    input_gpu,\n",
    "    W1_gpu,\n",
    "    b1_gpu,\n",
    "    hidden1_gpu,\n",
    "    W2_gpu,\n",
    "    b2_gpu,\n",
    "    hidden2_gpu,\n",
    "    W3_gpu,\n",
    "    b3_gpu,\n",
    "    output_gpu,\n",
    "    np.uint32(batch_size),\n",
    "    np.uint32(input_dim),\n",
    "    np.uint32(hidden1_dim),\n",
    "    np.uint32(hidden2_dim),\n",
    "    np.uint32(output_dim),\n",
    "    block=(threads_per_block, 1, 1),\n",
    "    grid=(blocks, 1, 1)\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# Retrieve and print the results.\n",
    "# -------------------------------\n",
    "cuda.memcpy_dtoh(hidden1_host, hidden1_gpu)\n",
    "cuda.memcpy_dtoh(hidden2_host, hidden2_gpu)\n",
    "cuda.memcpy_dtoh(output_host, output_gpu)\n",
    "\n",
    "print(\"Hidden layer 1 activations shape:\", hidden1_host.shape)\n",
    "print(\"Hidden layer 2 activations shape:\", hidden2_host.shape)\n",
    "print(\"Output layer (logits) shape:\", output_host.shape)\n",
    "print(\"Output logits for each sample:\")\n",
    "print(output_host)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Architecture\n",
    "\n",
    "Let the input be:\n",
    "$$\n",
    "\\mathbf{x} \\in \\mathbb{R}^{d} \\quad \\text{with } d=784 \\quad (\\text{MNIST image flattened})\n",
    "$$\n",
    "\n",
    "The network has three layers defined as follows:\n",
    "\n",
    "### Layer 1 (Input to Hidden1)\n",
    "\n",
    "- **Weights:**  \n",
    "  $$\n",
    "  W^{(1)} \\in \\mathbb{R}^{n_1 \\times d}\n",
    "  $$\n",
    "  Each element is \\( W^{(1)}_{ij} \\) with \\( i = 1, \\dots, n_1 \\) and \\( j = 1, \\dots, d \\).\n",
    "\n",
    "- **Biases:**  \n",
    "  $$\n",
    "  \\mathbf{b}^{(1)} \\in \\mathbb{R}^{n_1}\n",
    "  $$\n",
    "  with components \\( b^{(1)}_{i} \\).\n",
    "\n",
    "- **Pre-activation:**  \n",
    "  $$\n",
    "  \\mathbf{z}^{(1)} = W^{(1)} \\mathbf{x} + \\mathbf{b}^{(1)}\n",
    "  $$\n",
    "\n",
    "- **Activation (ReLU):**  \n",
    "  $$\n",
    "  \\mathbf{a}^{(1)} = \\sigma(\\mathbf{z}^{(1)}) \\quad \\text{with } \\sigma(z) = \\max(0, z)\n",
    "  $$\n",
    "\n",
    "### Layer 2 (Hidden1 to Hidden2)\n",
    "\n",
    "- **Weights:**  \n",
    "  $$\n",
    "  W^{(2)} \\in \\mathbb{R}^{n_2 \\times n_1}\n",
    "  $$\n",
    "  Each element is \\( W^{(2)}_{ij} \\) for \\( i = 1, \\dots, n_2 \\) and \\( j = 1, \\dots, n_1 \\).\n",
    "\n",
    "- **Biases:**  \n",
    "  $$\n",
    "  \\mathbf{b}^{(2)} \\in \\mathbb{R}^{n_2}\n",
    "  $$\n",
    "  with components \\( b^{(2)}_{i} \\).\n",
    "\n",
    "- **Pre-activation:**  \n",
    "  $$\n",
    "  \\mathbf{z}^{(2)} = W^{(2)} \\mathbf{a}^{(1)} + \\mathbf{b}^{(2)}\n",
    "  $$\n",
    "\n",
    "- **Activation (ReLU):**  \n",
    "  $$\n",
    "  \\mathbf{a}^{(2)} = \\sigma(\\mathbf{z}^{(2)}) \\quad \\text{with } \\sigma(z) = \\max(0, z)\n",
    "  $$\n",
    "\n",
    "### Layer 3 (Hidden2 to Output)\n",
    "\n",
    "- **Weights:**  \n",
    "  $$\n",
    "  W^{(3)} \\in \\mathbb{R}^{10 \\times n_2}\n",
    "  $$\n",
    "  Each element is \\( W^{(3)}_{ij} \\) for \\( i = 1, \\dots, 10 \\) and \\( j = 1, \\dots, n_2 \\).\n",
    "\n",
    "- **Biases:**  \n",
    "  $$\n",
    "  \\mathbf{b}^{(3)} \\in \\mathbb{R}^{10}\n",
    "  $$\n",
    "  with components \\( b^{(3)}_{i} \\).\n",
    "\n",
    "- **Pre-activation (Logits):**  \n",
    "  $$\n",
    "  \\mathbf{z}^{(3)} = W^{(3)} \\mathbf{a}^{(2)} + \\mathbf{b}^{(3)}\n",
    "  $$\n",
    "\n",
    "- **Output:**  \n",
    "  Often, a softmax is applied to \\(\\mathbf{z}^{(3)}\\) to obtain probabilities:\n",
    "  $$\n",
    "  \\hat{\\mathbf{y}} = \\operatorname{softmax}(\\mathbf{z}^{(3)}) \\quad \\text{where } \\hat{y}_i = \\frac{e^{z^{(3)}_i}}{\\sum_{k=1}^{10} e^{z^{(3)}_k}}\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "## Loss Function\n",
    "\n",
    "Assume we use the cross-entropy loss for classification. For a single training example with true label vector $\\mathbf{y}$ (one-hot encoded), the loss is:\n",
    "$$\n",
    "L = -\\sum_{i=1}^{10} y_i \\log(\\hat{y}_i)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Backpropagation\n",
    "\n",
    "Let the derivative of the loss with respect to any variable \\( u \\) be denoted by \\(\\frac{\\partial L}{\\partial u}\\). We define the error terms (or deltas) for each layer.\n",
    "\n",
    "### Output Layer (Layer 3)\n",
    "\n",
    "For the output layer using softmax and cross-entropy, the error is:\n",
    "$$\n",
    "\\delta^{(3)} = \\hat{\\mathbf{y}} - \\mathbf{y}\n",
    "$$\n",
    "\n",
    "The gradients for layer 3 are:\n",
    "\n",
    "- **Weights:**\n",
    "  $$\n",
    "  \\frac{\\partial L}{\\partial W^{(3)}} = \\delta^{(3)} \\, (\\mathbf{a}^{(2)})^T\n",
    "  $$\n",
    "\n",
    "- **Biases:**\n",
    "  $$\n",
    "  \\frac{\\partial L}{\\partial \\mathbf{b}^{(3)}} = \\delta^{(3)}\n",
    "  $$\n",
    "\n",
    "### Hidden Layer 2 (Layer 2)\n",
    "\n",
    "The error for layer 2 is computed by backpropagating the error from layer 3 and applying the derivative of the ReLU activation. Recall:\n",
    "$$\n",
    "\\sigma'(z) = \\begin{cases} \n",
    "1 & \\text{if } z > 0, \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Thus,\n",
    "$$\n",
    "\\delta^{(2)} = \\left( W^{(3)} \\right)^T \\delta^{(3)} \\circ \\sigma'(\\mathbf{z}^{(2)})\n",
    "$$\n",
    "where “\\(\\circ\\)” denotes element-wise multiplication.\n",
    "\n",
    "The gradients for layer 2 are:\n",
    "\n",
    "- **Weights:**\n",
    "  $$\n",
    "  \\frac{\\partial L}{\\partial W^{(2)}} = \\delta^{(2)} \\, (\\mathbf{a}^{(1)})^T\n",
    "  $$\n",
    "\n",
    "- **Biases:**\n",
    "  $$\n",
    "  \\frac{\\partial L}{\\partial \\mathbf{b}^{(2)}} = \\delta^{(2)}\n",
    "  $$\n",
    "\n",
    "### Hidden Layer 1 (Layer 1)\n",
    "\n",
    "Similarly, backpropagate the error to the first hidden layer:\n",
    "$$\n",
    "\\delta^{(1)} = \\left( W^{(2)} \\right)^T \\delta^{(2)} \\circ \\sigma'(\\mathbf{z}^{(1)})\n",
    "$$\n",
    "\n",
    "The gradients for layer 1 are:\n",
    "\n",
    "- **Weights:**\n",
    "  $$\n",
    "  \\frac{\\partial L}{\\partial W^{(1)}} = \\delta^{(1)} \\, (\\mathbf{x})^T\n",
    "  $$\n",
    "\n",
    "- **Biases:**\n",
    "  $$\n",
    "  \\frac{\\partial L}{\\partial \\mathbf{b}^{(1)}} = \\delta^{(1)}\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Forward Pass\n",
    "\n",
    "1. **Layer 1:**\n",
    "   $$\n",
    "   \\mathbf{z}^{(1)} = W^{(1)} \\mathbf{x} + \\mathbf{b}^{(1)}, \\quad \\mathbf{a}^{(1)} = \\max(0, \\mathbf{z}^{(1)})\n",
    "   $$\n",
    "\n",
    "2. **Layer 2:**\n",
    "   $$\n",
    "   \\mathbf{z}^{(2)} = W^{(2)} \\mathbf{a}^{(1)} + \\mathbf{b}^{(2)}, \\quad \\mathbf{a}^{(2)} = \\max(0, \\mathbf{z}^{(2)})\n",
    "   $$\n",
    "\n",
    "3. **Layer 3:**\n",
    "   $$\n",
    "   \\mathbf{z}^{(3)} = W^{(3)} \\mathbf{a}^{(2)} + \\mathbf{b}^{(3)}, \\quad \\hat{\\mathbf{y}} = \\operatorname{softmax}(\\mathbf{z}^{(3)})\n",
    "   $$\n",
    "\n",
    "### Backward Pass\n",
    "\n",
    "1. **Output Layer (Layer 3):**\n",
    "   $$\n",
    "   \\delta^{(3)} = \\hat{\\mathbf{y}} - \\mathbf{y}\n",
    "   $$\n",
    "   $$\n",
    "   \\frac{\\partial L}{\\partial W^{(3)}} = \\delta^{(3)} (\\mathbf{a}^{(2)})^T, \\quad \\frac{\\partial L}{\\partial \\mathbf{b}^{(3)}} = \\delta^{(3)}\n",
    "   $$\n",
    "\n",
    "2. **Hidden Layer 2 (Layer 2):**\n",
    "   $$\n",
    "   \\delta^{(2)} = \\left( W^{(3)} \\right)^T \\delta^{(3)} \\circ 1_{\\{\\mathbf{z}^{(2)} > 0\\}}\n",
    "   $$\n",
    "   $$\n",
    "   \\frac{\\partial L}{\\partial W^{(2)}} = \\delta^{(2)} (\\mathbf{a}^{(1)})^T, \\quad \\frac{\\partial L}{\\partial \\mathbf{b}^{(2)}} = \\delta^{(2)}\n",
    "   $$\n",
    "\n",
    "3. **Hidden Layer 1 (Layer 1):**\n",
    "   $$\n",
    "   \\delta^{(1)} = \\left( W^{(2)} \\right)^T \\delta^{(2)} \\circ 1_{\\{\\mathbf{z}^{(1)} > 0\\}}\n",
    "   $$\n",
    "   $$\n",
    "   \\frac{\\partial L}{\\partial W^{(1)}} = \\delta^{(1)} (\\mathbf{x})^T, \\quad \\frac{\\partial L}{\\partial \\mathbf{b}^{(1)}} = \\delta^{(1)}\n",
    "   $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_code = r\"\"\"\n",
    ".version 7.0\n",
    ".target sm_35\n",
    ".address_size 64\n",
    "\n",
    ".visible .entry trainStep_fb(\n",
    "    .param .u64 input_ptr,       // [batch_size x input_dim]\n",
    "    .param .u64 labels_ptr,      // [batch_size x output_dim] (one-hot)\n",
    "    .param .u64 hidden1_ptr,     // scratch for layer1 activations [batch_size x hidden1_dim]\n",
    "    .param .u64 hidden2_ptr,     // scratch for layer2 activations [batch_size x hidden2_dim]\n",
    "    .param .u64 output_ptr,      // scratch for layer3 outputs [batch_size x output_dim]\n",
    "    .param .u64 weights1_ptr,    // W1: [hidden1_dim x input_dim]\n",
    "    .param .u64 bias1_ptr,       // b1: [hidden1_dim]\n",
    "    .param .u64 weights2_ptr,    // W2: [hidden2_dim x hidden1_dim]\n",
    "    .param .u64 bias2_ptr,       // b2: [hidden2_dim]\n",
    "    .param .u64 weights3_ptr,    // W3: [output_dim x hidden2_dim]\n",
    "    .param .u64 bias3_ptr,       // b3: [output_dim]\n",
    "    .param .u32 batch_size,\n",
    "    .param .u32 input_dim,\n",
    "    .param .u32 hidden1_dim,\n",
    "    .param .u32 hidden2_dim,\n",
    "    .param .u32 output_dim,\n",
    "    .param .f32 learning_rate    // γ\n",
    ")\n",
    "{\n",
    "    // Allocate local memory for delta2 (δ^2) per thread.\n",
    "    .local .align 4 .f32 delta2_array[64];\n",
    "\n",
    "    // ------------------ Register Declarations ----------------------\n",
    "    .reg .u64    in_base, lab_base, h1_base, h2_base, out_base;\n",
    "    .reg .u64    w1_base, b1_base, w2_base, b2_base, w3_base, b3_base;\n",
    "    .reg .u32    bs, inputDim, hidden1Dim, hidden2Dim, outputDim;\n",
    "    .reg .f32    gamma;\n",
    "    .reg .u32    tid, blockId, blockDim, sampleIdx;\n",
    "    .reg .u32    i, j, k, l, idx, idx_byte;\n",
    "    .reg .u64    byte_offset;\n",
    "    .reg .u64    addr;\n",
    "    .reg .f32    temp, in_val, wt, bias_val, acc;\n",
    "    .reg .f32    a_val;\n",
    "    .reg .f32    delta3, delta2, delta1;\n",
    "    .reg .f32    label_val;\n",
    "    .reg .f32    a_h1, a_h2;\n",
    "    .reg .pred   p_exit;\n",
    "    .reg .u64    local_delta2_ptr;\n",
    "\n",
    "    // -------------------- Load Kernel Parameters ---------------------\n",
    "    ld.param.u64   in_base,   [input_ptr];\n",
    "    ld.param.u64   lab_base,  [labels_ptr];\n",
    "    ld.param.u64   h1_base,   [hidden1_ptr];\n",
    "    ld.param.u64   h2_base,   [hidden2_ptr];\n",
    "    ld.param.u64   out_base,  [output_ptr];\n",
    "    ld.param.u64   w1_base,   [weights1_ptr];\n",
    "    ld.param.u64   b1_base,   [bias1_ptr];\n",
    "    ld.param.u64   w2_base,   [weights2_ptr];\n",
    "    ld.param.u64   b2_base,   [bias2_ptr];\n",
    "    ld.param.u64   w3_base,   [weights3_ptr];\n",
    "    ld.param.u64   b3_base,   [bias3_ptr];\n",
    "    ld.param.u32   bs,        [batch_size];\n",
    "    ld.param.u32   inputDim,  [input_dim];\n",
    "    ld.param.u32   hidden1Dim, [hidden1_dim];\n",
    "    ld.param.u32   hidden2Dim, [hidden2_dim];\n",
    "    ld.param.u32   outputDim, [output_dim];\n",
    "    ld.param.f32   gamma,     [learning_rate];\n",
    "\n",
    "    // Convert pointer parameters to global address space.\n",
    "    cvta.global.u64 in_base, in_base;\n",
    "    cvta.global.u64 lab_base, lab_base;\n",
    "    cvta.global.u64 h1_base, h1_base;\n",
    "    cvta.global.u64 h2_base, h2_base;\n",
    "    cvta.global.u64 out_base, out_base;\n",
    "    cvta.global.u64 w1_base, w1_base;\n",
    "    cvta.global.u64 b1_base, b1_base;\n",
    "    cvta.global.u64 w2_base, w2_base;\n",
    "    cvta.global.u64 b2_base, b2_base;\n",
    "    cvta.global.u64 w3_base, w3_base;\n",
    "    cvta.global.u64 b3_base, b3_base;\n",
    "\n",
    "    // ------------------- Compute Sample Index ------------------------\n",
    "    mov.u32 tid, %tid.x;\n",
    "    mov.u32 blockId, %ctaid.x;\n",
    "    mov.u32 blockDim, %ntid.x;\n",
    "    mul.lo.u32 sampleIdx, blockId, blockDim;\n",
    "    add.u32 sampleIdx, sampleIdx, tid;\n",
    "    setp.ge.u32 p_exit, sampleIdx, bs;\n",
    "    @p_exit bra exit;\n",
    "\n",
    "    // ---------------------- Forward Pass -----------------------------\n",
    "    // Layer 1: a^(1) = ReLU(W1*x + b1)\n",
    "    mov.u32 j, 0;\n",
    "layer1_loop:\n",
    "    setp.ge.u32 p_exit, j, hidden1Dim;\n",
    "    @p_exit bra layer1_end;\n",
    "        mov.f32 acc, 0f00000000;\n",
    "        mov.u32 i, 0;\n",
    "    layer1_inner:\n",
    "        setp.ge.u32 p_exit, i, inputDim;\n",
    "        @p_exit bra layer1_inner_end;\n",
    "            // Compute index = j*inputDim + i\n",
    "            mul.lo.u32 idx, j, inputDim;\n",
    "            add.u32 idx, idx, i;\n",
    "            mul.lo.u32 idx_byte, idx, 4;\n",
    "            cvt.u64.u32 byte_offset, idx_byte;\n",
    "            add.u64 addr, w1_base, byte_offset;\n",
    "            ld.global.f32 wt, [addr];\n",
    "            // Load input: index = sampleIdx*inputDim + i\n",
    "            mul.lo.u32 idx, sampleIdx, inputDim;\n",
    "            add.u32 idx, idx, i;\n",
    "            mul.lo.u32 idx_byte, idx, 4;\n",
    "            cvt.u64.u32 byte_offset, idx_byte;\n",
    "            add.u64 addr, in_base, byte_offset;\n",
    "            ld.global.f32 in_val, [addr];\n",
    "            mul.f32 temp, wt, in_val;\n",
    "            add.f32 acc, acc, temp;\n",
    "            add.u32 i, i, 1;\n",
    "            bra layer1_inner;\n",
    "    layer1_inner_end:\n",
    "        // Add bias: b1[j]\n",
    "        mul.lo.u32 idx_byte, j, 4;\n",
    "        cvt.u64.u32 byte_offset, idx_byte;\n",
    "        add.u64 addr, b1_base, byte_offset;\n",
    "        ld.global.f32 bias_val, [addr];\n",
    "        add.f32 acc, acc, bias_val;\n",
    "        // Apply ReLU:\n",
    "        max.f32 acc, acc, 0f00000000;\n",
    "        // Store activation in hidden1: index = sampleIdx*hidden1Dim + j\n",
    "        mul.lo.u32 idx, sampleIdx, hidden1Dim;\n",
    "        add.u32 idx, idx, j;\n",
    "        mul.lo.u32 idx_byte, idx, 4;\n",
    "        cvt.u64.u32 byte_offset, idx_byte;\n",
    "        add.u64 addr, h1_base, byte_offset;\n",
    "        st.global.f32 [addr], acc;\n",
    "        add.u32 j, j, 1;\n",
    "        bra layer1_loop;\n",
    "layer1_end:\n",
    "\n",
    "    // Layer 2: a^(2) = ReLU(W2 * a^(1) + b2)\n",
    "    mov.u32 j, 0;\n",
    "layer2_loop:\n",
    "    setp.ge.u32 p_exit, j, hidden2Dim;\n",
    "    @p_exit bra layer2_end;\n",
    "        mov.f32 acc, 0f00000000;\n",
    "        mov.u32 i, 0;\n",
    "    layer2_inner:\n",
    "        setp.ge.u32 p_exit, i, hidden1Dim;\n",
    "        @p_exit bra layer2_inner_end;\n",
    "            // Compute index = j*hidden1Dim + i\n",
    "            mul.lo.u32 idx, j, hidden1Dim;\n",
    "            add.u32 idx, idx, i;\n",
    "            mul.lo.u32 idx_byte, idx, 4;\n",
    "            cvt.u64.u32 byte_offset, idx_byte;\n",
    "            add.u64 addr, w2_base, byte_offset;\n",
    "            ld.global.f32 wt, [addr];\n",
    "            // Load activation from hidden1: index = sampleIdx*hidden1Dim + i\n",
    "            mul.lo.u32 idx, sampleIdx, hidden1Dim;\n",
    "            add.u32 idx, idx, i;\n",
    "            mul.lo.u32 idx_byte, idx, 4;\n",
    "            cvt.u64.u32 byte_offset, idx_byte;\n",
    "            add.u64 addr, h1_base, byte_offset;\n",
    "            ld.global.f32 in_val, [addr];\n",
    "            mul.f32 temp, wt, in_val;\n",
    "            add.f32 acc, acc, temp;\n",
    "            add.u32 i, i, 1;\n",
    "            bra layer2_inner;\n",
    "    layer2_inner_end:\n",
    "        // Add bias: b2[j]\n",
    "        mul.lo.u32 idx_byte, j, 4;\n",
    "        cvt.u64.u32 byte_offset, idx_byte;\n",
    "        add.u64 addr, b2_base, byte_offset;\n",
    "        ld.global.f32 bias_val, [addr];\n",
    "        add.f32 acc, acc, bias_val;\n",
    "        // Apply ReLU:\n",
    "        max.f32 acc, acc, 0f00000000;\n",
    "        // Store activation in hidden2: index = sampleIdx*hidden2Dim + j\n",
    "        mul.lo.u32 idx, sampleIdx, hidden2Dim;\n",
    "        add.u32 idx, idx, j;\n",
    "        mul.lo.u32 idx_byte, idx, 4;\n",
    "        cvt.u64.u32 byte_offset, idx_byte;\n",
    "        add.u64 addr, h2_base, byte_offset;\n",
    "        st.global.f32 [addr], acc;\n",
    "        add.u32 j, j, 1;\n",
    "        bra layer2_loop;\n",
    "layer2_end:\n",
    "\n",
    "    // Layer 3: a^(3) = W3 * a^(2) + b3   (logits)\n",
    "    mov.u32 j, 0;\n",
    "layer3_loop:\n",
    "    setp.ge.u32 p_exit, j, outputDim;\n",
    "    @p_exit bra layer3_end;\n",
    "        mov.f32 acc, 0f00000000;\n",
    "        mov.u32 i, 0;\n",
    "    layer3_inner:\n",
    "        setp.ge.u32 p_exit, i, hidden2Dim;\n",
    "        @p_exit bra layer3_inner_end;\n",
    "            // Compute index = j*hidden2Dim + i\n",
    "            mul.lo.u32 idx, j, hidden2Dim;\n",
    "            add.u32 idx, idx, i;\n",
    "            mul.lo.u32 idx_byte, idx, 4;\n",
    "            cvt.u64.u32 byte_offset, idx_byte;\n",
    "            add.u64 addr, w3_base, byte_offset;\n",
    "            ld.global.f32 wt, [addr];\n",
    "            // Load activation from hidden2: index = sampleIdx*hidden2Dim + i\n",
    "            mul.lo.u32 idx, sampleIdx, hidden2Dim;\n",
    "            add.u32 idx, idx, i;\n",
    "            mul.lo.u32 idx_byte, idx, 4;\n",
    "            cvt.u64.u32 byte_offset, idx_byte;\n",
    "            add.u64 addr, h2_base, byte_offset;\n",
    "            ld.global.f32 in_val, [addr];\n",
    "            mul.f32 temp, wt, in_val;\n",
    "            add.f32 acc, acc, temp;\n",
    "            add.u32 i, i, 1;\n",
    "            bra layer3_inner;\n",
    "    layer3_inner_end:\n",
    "        // Add bias: b3[j]\n",
    "        mul.lo.u32 idx_byte, j, 4;\n",
    "        cvt.u64.u32 byte_offset, idx_byte;\n",
    "        add.u64 addr, b3_base, byte_offset;\n",
    "        ld.global.f32 bias_val, [addr];\n",
    "        add.f32 acc, acc, bias_val;\n",
    "        // Store output (logit) in global memory: index = sampleIdx*outputDim + j\n",
    "        mul.lo.u32 idx, sampleIdx, outputDim;\n",
    "        add.u32 idx, idx, j;\n",
    "        mul.lo.u32 idx_byte, idx, 4;\n",
    "        cvt.u64.u32 byte_offset, idx_byte;\n",
    "        add.u64 addr, out_base, byte_offset;\n",
    "        st.global.f32 [addr], acc;\n",
    "        add.u32 j, j, 1;\n",
    "        bra layer3_loop;\n",
    "layer3_end:\n",
    "\n",
    "    // ---------------- Backpropagation --------------------------\n",
    "    // ---------- Layer 3 BP ----------\n",
    "    // δ^3 = a^(3) - label; update bias b3 and weight matrix W3.\n",
    "    mov.u32 i, 0;\n",
    "layer3_bp_loop:\n",
    "    setp.ge.u32 p_exit, i, outputDim;\n",
    "    @p_exit bra layer3_bp_end;\n",
    "        // Load output a^(3)[i]\n",
    "        mul.lo.u32 idx_byte, i, 4;\n",
    "        cvt.u64.u32 byte_offset, idx_byte;\n",
    "        add.u64 addr, out_base, byte_offset;\n",
    "        ld.global.f32 a_val, [addr];\n",
    "        // Load label[i]\n",
    "        mul.lo.u32 idx_byte, i, 4;\n",
    "        cvt.u64.u32 byte_offset, idx_byte;\n",
    "        add.u64 addr, lab_base, byte_offset;\n",
    "        ld.global.f32 label_val, [addr];\n",
    "        sub.f32 delta3, a_val, label_val;\n",
    "        // Update bias b3[i]\n",
    "        mul.f32 temp, gamma, delta3;\n",
    "        neg.f32 temp, temp;\n",
    "        mul.lo.u32 idx_byte, i, 4;\n",
    "        cvt.u64.u32 byte_offset, idx_byte;\n",
    "        add.u64 addr, b3_base, byte_offset;\n",
    "        atom.global.add.f32 [addr], temp;\n",
    "        // Update weight matrix W3 for each hidden2 neuron j:\n",
    "        mov.u32 j, 0;\n",
    "    layer3_bp_inner_loop:\n",
    "        setp.ge.u32 p_exit, j, hidden2Dim;\n",
    "        @p_exit bra layer3_bp_inner_end;\n",
    "            // Load activation from hidden2: index = sampleIdx*hidden2Dim + j\n",
    "            mul.lo.u32 idx, sampleIdx, hidden2Dim;\n",
    "            add.u32 idx, idx, j;\n",
    "            mul.lo.u32 idx_byte, idx, 4;\n",
    "            cvt.u64.u32 byte_offset, idx_byte;\n",
    "            add.u64 addr, h2_base, byte_offset;\n",
    "            ld.global.f32 a_h2, [addr];\n",
    "            mul.f32 temp, delta3, a_h2;\n",
    "            mul.f32 temp, gamma, temp;\n",
    "            neg.f32 temp, temp;\n",
    "            // Update weight W3[i,j]: index = i*hidden2Dim + j\n",
    "            mul.lo.u32 idx, i, hidden2Dim;\n",
    "            add.u32 idx, idx, j;\n",
    "            mul.lo.u32 idx_byte, idx, 4;\n",
    "            cvt.u64.u32 byte_offset, idx_byte;\n",
    "            add.u64 addr, w3_base, byte_offset;\n",
    "            atom.global.add.f32 [addr], temp;\n",
    "            add.u32 j, j, 1;\n",
    "            bra layer3_bp_inner_loop;\n",
    "    layer3_bp_inner_end:\n",
    "        add.u32 i, i, 1;\n",
    "        bra layer3_bp_loop;\n",
    "layer3_bp_end:\n",
    "\n",
    "    // ---------- Layer 2 BP ----------\n",
    "    // δ^2 = (sum_i W3[i,j]*δ^3[i]) * ReLU'(a^(2)[j]); update b2 and W2.\n",
    "    mov.u32 j, 0;\n",
    "layer2_bp_loop:\n",
    "    setp.ge.u32 p_exit, j, hidden2Dim;\n",
    "    @p_exit bra layer2_bp_end;\n",
    "        mov.f32 delta2, 0f00000000;\n",
    "        mov.u32 i, 0;\n",
    "    layer2_bp_inner_loop:\n",
    "        setp.ge.u32 p_exit, i, outputDim;\n",
    "        @p_exit bra layer2_bp_inner_end;\n",
    "            // For W3: index = i*hidden2Dim + j\n",
    "            mul.lo.u32 idx, i, hidden2Dim;\n",
    "            add.u32 idx, idx, j;\n",
    "            mul.lo.u32 idx_byte, idx, 4;\n",
    "            cvt.u64.u32 byte_offset, idx_byte;\n",
    "            add.u64 addr, w3_base, byte_offset;\n",
    "            ld.global.f32 wt, [addr];\n",
    "            // Recompute δ^3 for neuron i\n",
    "            mul.lo.u32 idx_byte, i, 4;\n",
    "            cvt.u64.u32 byte_offset, idx_byte;\n",
    "            add.u64 addr, out_base, byte_offset;\n",
    "            ld.global.f32 a_val, [addr];\n",
    "            mul.lo.u32 idx_byte, i, 4;\n",
    "            cvt.u64.u32 byte_offset, idx_byte;\n",
    "            add.u64 addr, lab_base, byte_offset;\n",
    "            ld.global.f32 label_val, [addr];\n",
    "            sub.f32 delta3, a_val, label_val;\n",
    "            mul.f32 temp, wt, delta3;\n",
    "            add.f32 delta2, delta2, temp;\n",
    "            add.u32 i, i, 1;\n",
    "            bra layer2_bp_inner_loop;\n",
    "    layer2_bp_inner_end:\n",
    "        // Apply ReLU derivative on a^(2)[j]:\n",
    "        mul.lo.u32 idx_byte, j, 4;\n",
    "        cvt.u64.u32 byte_offset, idx_byte;\n",
    "        add.u64 addr, h2_base, byte_offset;\n",
    "        ld.global.f32 a_val, [addr];\n",
    "        setp.le.f32 p_exit, a_val, 0f00000000;\n",
    "        @p_exit mov.f32 delta2, 0f00000000;\n",
    "        // Store δ^2[j] in local memory\n",
    "        mul.lo.u32 idx_byte, j, 4;\n",
    "        cvta.local.u64 local_delta2_ptr, delta2_array;\n",
    "        add.u64 addr, local_delta2_ptr, idx_byte;\n",
    "        st.local.f32 [addr], delta2;\n",
    "        // Update bias b2[j]\n",
    "        mul.f32 temp, gamma, delta2;\n",
    "        neg.f32 temp, temp;\n",
    "        mul.lo.u32 idx_byte, j, 4;\n",
    "        cvt.u64.u32 byte_offset, idx_byte;\n",
    "        add.u64 addr, b2_base, byte_offset;\n",
    "        atom.global.add.f32 [addr], temp;\n",
    "        // Update W2: for each hidden1 neuron k\n",
    "        mov.u32 k, 0;\n",
    "    layer2_bp_inner2_loop:\n",
    "        setp.ge.u32 p_exit, k, hidden1Dim;\n",
    "        @p_exit bra layer2_bp_inner2_end;\n",
    "            // Load activation from hidden1: index = sampleIdx*hidden1Dim + k\n",
    "            mul.lo.u32 idx, sampleIdx, hidden1Dim;\n",
    "            add.u32 idx, idx, k;\n",
    "            mul.lo.u32 idx_byte, idx, 4;\n",
    "            cvt.u64.u32 byte_offset, idx_byte;\n",
    "            add.u64 addr, h1_base, byte_offset;\n",
    "            ld.global.f32 a_h1, [addr];\n",
    "            mul.f32 temp, delta2, a_h1;\n",
    "            mul.f32 temp, gamma, temp;\n",
    "            neg.f32 temp, temp;\n",
    "            // Update weight W2[j,k]: index = j*hidden1Dim + k\n",
    "            mul.lo.u32 idx, j, hidden1Dim;\n",
    "            add.u32 idx, idx, k;\n",
    "            mul.lo.u32 idx_byte, idx, 4;\n",
    "            cvt.u64.u32 byte_offset, idx_byte;\n",
    "            add.u64 addr, w2_base, byte_offset;\n",
    "            atom.global.add.f32 [addr], temp;\n",
    "            add.u32 k, k, 1;\n",
    "            bra layer2_bp_inner2_loop;\n",
    "    layer2_bp_inner2_end:\n",
    "        add.u32 j, j, 1;\n",
    "        bra layer2_bp_loop;\n",
    "layer2_bp_end:\n",
    "\n",
    "    // ---------- Layer 1 BP ----------\n",
    "    // δ^1 = (sum_j W2[j,k]*δ^2[j]) * ReLU'(a^(1)[k]); update b1 and W1.\n",
    "    mov.u32 k, 0;\n",
    "layer1_bp_loop:\n",
    "    setp.ge.u32 p_exit, k, hidden1Dim;\n",
    "    @p_exit bra layer1_bp_end;\n",
    "        mov.f32 delta1, 0f00000000;\n",
    "        mov.u32 j, 0;\n",
    "    layer1_bp_inner_loop:\n",
    "        setp.ge.u32 p_exit, j, hidden2Dim;\n",
    "        @p_exit bra layer1_bp_inner_end;\n",
    "            // Compute index for W2[j,k]: j*hidden1Dim + k\n",
    "            mul.lo.u32 idx, j, hidden1Dim;\n",
    "            add.u32 idx, idx, k;\n",
    "            mul.lo.u32 idx_byte, idx, 4;\n",
    "            cvt.u64.u32 byte_offset, idx_byte;\n",
    "            add.u64 addr, w2_base, byte_offset;\n",
    "            ld.global.f32 wt, [addr];\n",
    "            // Load δ^2[j] from local memory\n",
    "            mul.lo.u32 idx_byte, j, 4;\n",
    "            cvta.local.u64 local_delta2_ptr, delta2_array;\n",
    "            add.u64 addr, local_delta2_ptr, idx_byte;\n",
    "            ld.local.f32 delta2, [addr];\n",
    "            mul.f32 temp, wt, delta2;\n",
    "            add.f32 delta1, delta1, temp;\n",
    "            add.u32 j, j, 1;\n",
    "            bra layer1_bp_inner_loop;\n",
    "    layer1_bp_inner_end:\n",
    "        // Apply ReLU derivative on a^(1)[k]:\n",
    "        mul.lo.u32 idx_byte, k, 4;\n",
    "        cvt.u64.u32 byte_offset, idx_byte;\n",
    "        add.u64 addr, h1_base, byte_offset;\n",
    "        ld.global.f32 a_val, [addr];\n",
    "        setp.le.f32 p_exit, a_val, 0f00000000;\n",
    "        @p_exit mov.f32 delta1, 0f00000000;\n",
    "        // Update bias b1[k]\n",
    "        mul.f32 temp, gamma, delta1;\n",
    "        neg.f32 temp, temp;\n",
    "        mul.lo.u32 idx_byte, k, 4;\n",
    "        cvt.u64.u32 byte_offset, idx_byte;\n",
    "        add.u64 addr, b1_base, byte_offset;\n",
    "        atom.global.add.f32 [addr], temp;\n",
    "        // Update W1: for each input neuron l\n",
    "        mov.u32 l, 0;\n",
    "    layer1_bp_inner2_loop:\n",
    "        setp.ge.u32 p_exit, l, inputDim;\n",
    "        @p_exit bra layer1_bp_inner2_end;\n",
    "            // Load input: index = sampleIdx*inputDim + l\n",
    "            mul.lo.u32 idx, sampleIdx, inputDim;\n",
    "            add.u32 idx, idx, l;\n",
    "            mul.lo.u32 idx_byte, idx, 4;\n",
    "            cvt.u64.u32 byte_offset, idx_byte;\n",
    "            add.u64 addr, in_base, byte_offset;\n",
    "            ld.global.f32 in_val, [addr];\n",
    "            mul.f32 temp, delta1, in_val;\n",
    "            mul.f32 temp, gamma, temp;\n",
    "            neg.f32 temp, temp;\n",
    "            // Update weight W1[k,l]: index = k*inputDim + l\n",
    "            mul.lo.u32 idx, k, inputDim;\n",
    "            add.u32 idx, idx, l;\n",
    "            mul.lo.u32 idx_byte, idx, 4;\n",
    "            cvt.u64.u32 byte_offset, idx_byte;\n",
    "            add.u64 addr, w1_base, byte_offset;\n",
    "            atom.global.add.f32 [addr], temp;\n",
    "            add.u32 l, l, 1;\n",
    "            bra layer1_bp_inner2_loop;\n",
    "    layer1_bp_inner2_end:\n",
    "        add.u32 k, k, 1;\n",
    "        bra layer1_bp_loop;\n",
    "layer1_bp_end:\n",
    "\n",
    "exit:\n",
    "    ret;\n",
    "}\n",
    "\"\"\"\n",
    "update_code = r\"\"\"\n",
    ".version 7.0\n",
    ".target sm_35\n",
    ".address_size 64\n",
    "\n",
    ".visible .entry trainStep_update(\n",
    "    .param .u64 dW1_buf,   // [batch_size x (hidden1_dim * input_dim)]\n",
    "    .param .u64 db1_buf,   // [batch_size x hidden1_dim]\n",
    "    .param .u64 dW2_buf,   // [batch_size x (hidden2_dim * hidden1_dim)]\n",
    "    .param .u64 db2_buf,   // [batch_size x hidden2_dim]\n",
    "    .param .u64 dW3_buf,   // [batch_size x (output_dim * hidden2_dim)]\n",
    "    .param .u64 db3_buf,   // [batch_size x output_dim]\n",
    "    .param .u64 weights1_ptr, // W1: [hidden1_dim x input_dim]\n",
    "    .param .u64 bias1_ptr,    // b1: [hidden1_dim]\n",
    "    .param .u64 weights2_ptr, // W2: [hidden2_dim x hidden1_dim]\n",
    "    .param .u64 bias2_ptr,    // b2: [hidden2_dim]\n",
    "    .param .u64 weights3_ptr, // W3: [output_dim x hidden2_dim]\n",
    "    .param .u64 bias3_ptr,    // b3: [output_dim]\n",
    "    .param .u32 batch_size,\n",
    "    .param .u32 input_dim,\n",
    "    .param .u32 hidden1_dim,\n",
    "    .param .u32 hidden2_dim,\n",
    "    .param .u32 output_dim,\n",
    "    .param .f32 learning_rate\n",
    ")\n",
    "{\n",
    "    // ---------------------------------------------------------------\n",
    "    // Declare registers.\n",
    "    // ---------------------------------------------------------------\n",
    "    .reg .u64   dW1_base, db1_base, dW2_base, db2_base, dW3_base, db3_base;\n",
    "    .reg .u64   w1_base, b1_base, w2_base, b2_base, w3_base, b3_base;\n",
    "    .reg .u32   bs, inputDim, hidden1Dim, hidden2Dim, outputDim;\n",
    "    .reg .f32   lr;\n",
    "    .reg .u32   idx, sample;\n",
    "    .reg .f32   grad_sum, grad_avg;\n",
    "    .reg .f32   old_weight;\n",
    "    .reg .u64   byte_offset, addr;\n",
    "    .reg .u32   i;\n",
    "    .reg .pred  p_exit;\n",
    "\n",
    "    // ---------------------------------------------------------------\n",
    "    // Load parameters.\n",
    "    // ---------------------------------------------------------------\n",
    "    ld.param.u64 dW1_base, [dW1_buf];\n",
    "    ld.param.u64 db1_base, [db1_buf];\n",
    "    ld.param.u64 dW2_base, [dW2_buf];\n",
    "    ld.param.u64 db2_base, [db2_buf];\n",
    "    ld.param.u64 dW3_base, [dW3_buf];\n",
    "    ld.param.u64 db3_base, [db3_buf];\n",
    "    ld.param.u64 w1_base, [weights1_ptr];\n",
    "    ld.param.u64 b1_base, [bias1_ptr];\n",
    "    ld.param.u64 w2_base, [weights2_ptr];\n",
    "    ld.param.u64 b2_base, [bias2_ptr];\n",
    "    ld.param.u64 w3_base, [weights3_ptr];\n",
    "    ld.param.u64 b3_base, [bias3_ptr];\n",
    "    ld.param.u32 bs, [batch_size];\n",
    "    ld.param.u32 inputDim, [input_dim];\n",
    "    ld.param.u32 hidden1Dim, [hidden1_dim];\n",
    "    ld.param.u32 hidden2Dim, [hidden2_dim];\n",
    "    ld.param.u32 outputDim, [output_dim];\n",
    "    ld.param.f32 lr, [learning_rate];\n",
    "\n",
    "    // ---------------------------------------------------------------\n",
    "    // Example: Update W1.\n",
    "    // Loop over each element in W1 (total elements = hidden1_dim * input_dim).\n",
    "    mov.u32 idx, 0;\n",
    "W1_update_loop:\n",
    "    setp.ge.u32 p_exit, idx, hidden1Dim * inputDim;\n",
    "    @p_exit bra W1_update_end;\n",
    "       // Initialize grad_sum = 0.\n",
    "       mov.f32 grad_sum, 0f00000000;\n",
    "       mov.u32 sample, 0;\n",
    "    W1_reduce:\n",
    "       setp.ge.u32 p_exit, sample, bs;\n",
    "       @p_exit bra W1_reduce_end;\n",
    "          // Compute address for element 'idx' in the gradient buffer for sample.\n",
    "          // Each sample’s gradients are stored contiguously.\n",
    "          mul.lo.u32 i, sample, hidden1Dim;\n",
    "          mul.lo.u32 i, i, inputDim;\n",
    "          add.u32 i, i, idx;\n",
    "          mul.lo.u32 i, i, 4;\n",
    "          cvt.u64.u32 byte_offset, i;\n",
    "          add.u64 addr, dW1_base, byte_offset;\n",
    "          ld.global.f32 temp, [addr];\n",
    "          add.f32 grad_sum, grad_sum, temp;\n",
    "          add.u32 sample, sample, 1;\n",
    "          bra W1_reduce;\n",
    "    W1_reduce_end:\n",
    "       // Average gradient = grad_sum / bs.\n",
    "       div.f32 grad_avg, grad_sum, __uint2float_rn(bs);\n",
    "       // Update weight:\n",
    "       mul.lo.u32 i, idx, 4;\n",
    "       cvt.u64.u32 byte_offset, i;\n",
    "       add.u64 addr, w1_base, byte_offset;\n",
    "       ld.global.f32 old_weight, [addr];\n",
    "       mul.f32 temp, lr, grad_avg;\n",
    "       sub.f32 old_weight, old_weight, temp;\n",
    "       st.global.f32 [addr], old_weight;\n",
    "       add.u32 idx, idx, 1;\n",
    "       bra W1_update_loop;\n",
    "W1_update_end:\n",
    "    // Similar update loops must be written for db1, W2, b2, W3, and b3.\n",
    "    ret;\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sadly, this doesn't work, so I instead implement it in CUDA to save time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed batch 1\n",
      "Completed batch 2\n",
      "Completed batch 3\n",
      "Completed batch 4\n",
      "Completed batch 5\n",
      "Completed batch 6\n",
      "Completed batch 7\n",
      "Completed batch 8\n",
      "Completed batch 9\n",
      "Completed batch 10\n",
      "Training complete.\n",
      "Updated weights and biases:\n",
      "W1: [[ 0.00437199 -0.00232274  0.01523101 ...  0.00399005  0.00060943\n",
      "  -0.00379807]\n",
      " [ 0.00448643 -0.0070263   0.01537438 ...  0.00932618  0.00631156\n",
      "   0.00498502]\n",
      " [-0.0053929   0.00048365 -0.0018097  ...  0.00558358  0.00576927\n",
      "   0.00641882]\n",
      " ...\n",
      " [ 0.00283663 -0.00596519  0.01930408 ...  0.01084203  0.00051508\n",
      "   0.00443041]\n",
      " [-0.00656595  0.00544863 -0.01342957 ...  0.00244525  0.00281238\n",
      "   0.00508942]\n",
      " [-0.00308127  0.003688    0.00730584 ... -0.00074348 -0.00059138\n",
      "   0.01014786]]\n",
      "b1: [ 3.33784777e-03  1.61030708e-04 -2.57293601e-03  1.59598663e-02\n",
      " -6.32680440e-03 -6.16802415e-03  9.86100826e-03  7.18621491e-03\n",
      "  4.82773408e-03  1.93283800e-03 -5.34211705e-03 -8.81280005e-03\n",
      " -2.07232051e-02  5.43789798e-03 -2.43395261e-04  1.27090816e-03\n",
      "  8.52138549e-03 -3.96470586e-03 -1.03841233e-03 -1.42593670e-03\n",
      " -1.85723621e-02 -1.00023337e-02  1.59750495e-03 -2.00569090e-02\n",
      " -8.92565213e-03 -1.34157296e-02  2.47728657e-02 -3.54480348e-03\n",
      " -8.92724004e-03  1.29364040e-02  6.72708498e-03  6.24245324e-04\n",
      " -2.16676481e-03 -1.42410490e-05  3.58179212e-04 -1.02687599e-02\n",
      "  1.13865845e-02 -4.38366458e-03  1.63443554e-02  5.35335904e-03\n",
      "  1.16527900e-02  3.30204666e-02 -2.32083374e-03 -1.85955572e-03\n",
      " -8.93833395e-03 -4.22667246e-03 -6.16111467e-03  1.30624473e-02\n",
      " -1.56049924e-02  1.61534324e-02 -1.63643565e-02 -1.38353175e-02\n",
      "  4.91380878e-03  5.77057805e-03 -4.36331891e-03 -6.38201600e-03\n",
      "  1.38562722e-02 -1.58435777e-02 -5.59565332e-03  2.00506467e-02\n",
      "  2.07687300e-02 -1.31031685e-02  1.19539502e-03 -1.51020782e-02\n",
      "  7.00391224e-03 -1.39888860e-02 -6.70012645e-03 -8.60881060e-03\n",
      "  1.27847251e-02 -8.03068280e-03  4.04027487e-05  3.32787097e-03\n",
      "  3.35494959e-04  3.20010982e-03  7.64395529e-03 -1.05376716e-03\n",
      " -1.34713796e-03 -1.62505675e-02  1.67309176e-02 -3.84400529e-03\n",
      " -4.95872321e-03  5.78490458e-03 -5.49402041e-03 -1.47538341e-03\n",
      "  7.47327972e-03 -9.27874912e-03  1.19821331e-03  4.33997856e-03\n",
      " -2.02472936e-02  1.66289252e-03 -9.60845221e-03 -6.82053668e-03\n",
      " -3.18841003e-02  1.91168506e-02  2.04824097e-03 -5.11063170e-03\n",
      "  1.03701605e-03  2.57983934e-02  9.55693889e-03  6.41016057e-03\n",
      "  1.19575597e-02 -7.19139213e-03 -6.23347051e-03  1.18819682e-03\n",
      "  1.51574779e-02 -3.70870897e-04  1.20042628e-02 -8.35872721e-03\n",
      " -1.46545256e-02 -1.05169797e-02 -2.57571414e-03  1.96346492e-02\n",
      " -8.22037109e-04 -7.03311199e-03 -1.72568054e-03 -4.84561734e-03\n",
      " -1.32357720e-02 -1.04230111e-02  1.81706611e-03 -2.47804318e-02\n",
      " -1.60205748e-03  2.77073937e-03  4.86917514e-03 -5.82291652e-03\n",
      " -2.55241198e-03  7.93476556e-06 -2.06436832e-02  1.66899022e-02]\n",
      "W2: [[-0.00352136  0.01296526  0.0159267  ...  0.01144765  0.00822731\n",
      "   0.03083548]\n",
      " [-0.02604086  0.01229198 -0.00947128 ...  0.00903163  0.00249507\n",
      "  -0.00566542]\n",
      " [ 0.01005019  0.0006454  -0.0191979  ... -0.01305598 -0.00977136\n",
      "   0.00346695]\n",
      " ...\n",
      " [-0.00784815 -0.00892775 -0.01146137 ... -0.00585353 -0.00060362\n",
      "   0.00559678]\n",
      " [-0.00493968  0.01216598  0.0127274  ... -0.00352951  0.00879556\n",
      "   0.0212022 ]\n",
      " [ 0.00264657  0.01139282  0.00940705 ...  0.00477315  0.00477252\n",
      "  -0.00252347]]\n",
      "b2: [-1.7255504e-02 -7.6844953e-03  9.3288422e-03 -2.4429875e-05\n",
      "  5.5109337e-03  1.0465133e-02 -7.7533433e-03  2.6983155e-03\n",
      "  1.2196261e-03 -2.1249656e-02  5.0000185e-03  1.6433353e-02\n",
      "  3.8919393e-06  2.9646761e-03 -6.3014422e-03 -7.5570811e-03\n",
      "  3.4375470e-03  1.8545039e-02  1.6078385e-02  6.6716024e-03\n",
      "  1.6289366e-02  1.3327992e-03  9.0662912e-03  4.3651816e-03\n",
      " -5.0386772e-03  9.6350657e-03 -1.3791003e-04  4.6850622e-04\n",
      " -1.6586130e-03  1.1460588e-03  1.2431921e-03 -3.1705308e-03\n",
      " -4.5605483e-03  4.4097183e-03  8.9897774e-03 -1.6510820e-03\n",
      " -5.2757557e-03  5.4448429e-03  9.6826599e-04 -9.0118591e-03\n",
      " -1.2118975e-02 -8.7893344e-03 -2.4807692e-04 -1.5328172e-02\n",
      " -4.7090393e-03  4.1405000e-03  4.9818965e-04 -9.6552931e-03\n",
      "  2.7796491e-03 -6.0414150e-03 -2.2259913e-03 -6.1366544e-03\n",
      "  1.1445009e-02  6.8056741e-04 -1.4785875e-02  1.7069826e-02\n",
      " -1.5043201e-02  2.5081402e-03  6.4714917e-04  3.4714311e-03\n",
      "  1.7313925e-03 -1.5102536e-02 -1.2901827e-02 -1.6737426e-02]\n",
      "W3: [[ 1.80982947e-02 -1.56876119e-03  2.09686370e-03  1.35368388e-02\n",
      "   8.02092347e-03 -1.02349138e-03  4.68458794e-03  8.35654791e-04\n",
      "  -1.30392555e-02  5.13038412e-03  1.40958361e-03  2.84868432e-03\n",
      "  -3.63492128e-03 -1.19963214e-02 -1.83625205e-03 -4.44728183e-03\n",
      "  -1.10809971e-02 -1.74577173e-03 -1.38585707e-02  1.17226411e-02\n",
      "  -1.45514868e-02  2.54357234e-03  8.07998236e-03 -2.11503040e-02\n",
      "   1.24030979e-03 -1.39873018e-02 -7.11673032e-03 -1.04777748e-02\n",
      "  -8.92791059e-03  2.57590879e-03  9.09213908e-03 -1.04104271e-02\n",
      "  -9.85831674e-03  1.32269626e-02 -4.19605151e-03  7.91185908e-03\n",
      "  -2.16945354e-02  3.03397351e-03 -1.38854445e-03  1.57246217e-02\n",
      "  -5.00009255e-03  1.78338476e-02  1.50101325e-02 -1.42675184e-03\n",
      "   2.02406403e-02  9.26968176e-03 -5.91605029e-04  1.44505100e-02\n",
      "  -1.16259083e-02  6.64372277e-03  1.21863531e-02 -1.24140307e-02\n",
      "   1.66032859e-03 -4.65187710e-03  4.63810517e-03 -2.50225584e-03\n",
      "   5.45960478e-03  1.32932253e-02  2.29353039e-03 -1.24393841e-02\n",
      "   1.00420043e-02  1.08297274e-03  9.69494507e-03  2.87304632e-03]\n",
      " [ 1.07663535e-02  1.59821399e-02  9.74189211e-03 -2.17342805e-02\n",
      "  -4.03216807e-03 -7.10224546e-03 -2.18416322e-02  1.22300256e-02\n",
      "  -1.85262021e-02  6.33246731e-03  1.36830565e-02  4.68111923e-03\n",
      "  -1.65089183e-02 -9.02641844e-03 -2.16506654e-03  6.94986526e-03\n",
      "   1.62223037e-02 -4.05486999e-03  9.39928368e-03 -7.85650127e-03\n",
      "   1.00670327e-02  5.43288887e-04  5.31268585e-03  6.69135829e-04\n",
      "   7.52693182e-03 -1.90722607e-02 -1.54291978e-02 -1.16111273e-02\n",
      "   1.02832261e-02  5.87854767e-03  1.15384432e-02  3.06591368e-03\n",
      "  -1.01332190e-02  1.86530745e-03 -1.46191847e-02 -9.71847028e-03\n",
      "  -8.86632316e-03 -1.10970018e-03 -6.64168363e-03  2.77776108e-03\n",
      "  -5.43796318e-03  1.16770798e-02  1.14440313e-02 -6.76451484e-04\n",
      "  -5.09643927e-03  1.71173699e-02 -1.66564668e-03 -1.88840216e-03\n",
      "   6.12773979e-03  2.32675578e-03  1.07039111e-02 -3.82900180e-04\n",
      "  -5.51814446e-03 -1.56548098e-02  3.98321403e-03 -5.37768425e-03\n",
      "   2.05517244e-02  5.44635206e-03 -1.21364808e-02  2.14083064e-02\n",
      "   1.19576079e-03  1.35184983e-02 -6.35238504e-03  7.64487451e-03]\n",
      " [ 9.43940307e-04  4.16260166e-03 -1.19768260e-02  1.68592688e-02\n",
      "   1.46000264e-02 -6.41908869e-03 -4.17532818e-03  3.79034341e-03\n",
      "   3.80663527e-03  1.35262078e-02  1.17928674e-03 -1.40348962e-02\n",
      "   5.35652274e-03 -1.80598069e-02  1.41779874e-02  5.92748076e-03\n",
      "   9.54706687e-03  1.19408695e-02 -1.19784614e-02 -2.96628638e-03\n",
      "   1.89616960e-02  1.58567447e-02 -8.08760989e-03 -1.14163309e-02\n",
      "   8.53161048e-03  2.17736904e-02  3.07206134e-03  1.18541131e-02\n",
      "  -1.05357271e-04  1.53626204e-02  4.29105107e-03 -7.48675410e-03\n",
      "  -4.26119938e-03  2.76733277e-04  1.28924474e-02 -1.66234616e-02\n",
      "   9.60271154e-03  1.06542241e-02 -8.95672885e-04 -6.85123820e-03\n",
      "  -5.75206894e-03  4.85025300e-03 -4.94094472e-03 -1.75217520e-02\n",
      "   1.12865753e-02  9.56555188e-04 -3.04829166e-03 -1.01841344e-02\n",
      "  -9.19334497e-03  4.41571744e-03  6.88148371e-04  2.26208251e-02\n",
      "   4.24478529e-03  3.04324832e-03 -4.78835125e-03  2.73807999e-03\n",
      "   4.14745696e-03  1.15337707e-02 -5.94056863e-03 -2.21038442e-02\n",
      "  -6.32216129e-03 -2.39124661e-03 -9.08810034e-05 -7.19845062e-03]\n",
      " [-3.01013398e-03 -9.03449394e-03  1.50866415e-02 -2.03559548e-03\n",
      "  -6.82830252e-03 -2.72406708e-03  2.23224070e-02 -5.00867609e-04\n",
      "  -8.91349220e-04  7.07976427e-03 -5.76634286e-03 -4.88718273e-04\n",
      "  -4.28922148e-03  1.50758782e-02  3.85501795e-03  1.57663552e-03\n",
      "   2.16671750e-02 -1.89630827e-03  9.94223543e-03  7.57046137e-03\n",
      "  -4.87988163e-03 -9.56848636e-03 -3.49069480e-03 -3.02793016e-03\n",
      "   1.07799228e-02  1.91446030e-04  1.17423236e-02 -7.74126081e-03\n",
      "  -4.49829409e-03 -4.52596415e-03  2.28718645e-03 -1.02345832e-02\n",
      "  -1.75806100e-03 -1.92376785e-02 -1.10645138e-03 -6.42478582e-04\n",
      "  -2.03880779e-02 -1.11992629e-02 -4.00772318e-03  4.25598957e-03\n",
      "   1.95831573e-03  1.71623705e-03  9.85947438e-03 -8.20288900e-04\n",
      "  -6.82680728e-03  4.24869498e-03  2.40834635e-02  1.72913298e-02\n",
      "   7.31762126e-03  1.64782405e-02  4.25129384e-03 -5.48168086e-04\n",
      "   1.10382494e-02 -8.30210932e-03  2.32309947e-04 -2.86143553e-03\n",
      "   2.66167708e-03 -1.02588506e-02  6.57313596e-03 -1.18997658e-03\n",
      "   4.12927289e-03  1.68064982e-02  9.69642249e-04 -2.64383899e-03]\n",
      " [-6.90588076e-03 -5.42694516e-03 -4.88867424e-03  1.06145684e-02\n",
      "   3.79968027e-04 -2.65172939e-03 -3.48786265e-03  2.24053785e-02\n",
      "  -5.14724488e-05  2.00951216e-03 -7.71958102e-03  3.72571964e-03\n",
      "   1.18976468e-02 -9.15778335e-03 -4.48083412e-03  4.29193443e-03\n",
      "  -5.28356759e-03 -2.10631965e-03 -8.92959163e-03  2.10304675e-03\n",
      "   3.74415517e-03  1.56242475e-02 -6.05952274e-03  1.64423212e-02\n",
      "  -3.82275227e-03  6.94732807e-05  1.65314525e-02  3.92266503e-03\n",
      "  -1.08158551e-02  6.93431066e-04  4.61617531e-03 -1.35696121e-03\n",
      "   1.36228623e-02 -2.12183164e-04  1.62445735e-02 -1.48113770e-02\n",
      "  -1.81366212e-03 -4.58693458e-03 -2.31010304e-03 -9.72490665e-03\n",
      "  -1.36051411e-02  5.65398997e-03  3.19533539e-03 -2.06714263e-03\n",
      "   1.42878736e-03 -6.42243680e-03  2.01710388e-02  4.30305814e-03\n",
      "  -7.90028367e-03  5.88036841e-03 -6.55108737e-03  1.48604307e-02\n",
      "  -1.52795436e-02 -8.58126150e-04  1.03059616e-02  1.85491052e-03\n",
      "   4.41942317e-03  1.28536113e-02 -3.83293652e-03 -2.90363969e-04\n",
      "   1.18136369e-02  4.61481651e-03  2.50578835e-03  4.98599187e-03]\n",
      " [ 1.19042797e-02  1.68298595e-02 -5.46368444e-03 -8.04996490e-03\n",
      "  -1.93945318e-02  9.69171757e-04 -1.18937753e-02 -7.44322460e-06\n",
      "   2.34763566e-02  1.84297506e-02 -2.04589665e-02 -9.00915451e-03\n",
      "  -8.22453946e-03  5.94099611e-03  7.37260003e-03 -1.22172581e-02\n",
      "  -9.67841223e-03 -8.02163687e-03 -5.82202617e-03  9.40957479e-03\n",
      "   1.03160238e-03  7.82958791e-03 -2.37458423e-02  9.86425858e-03\n",
      "  -7.75115215e-04  9.30020865e-03  8.85260385e-03  4.41365549e-03\n",
      "   1.84917040e-02  1.26535259e-02 -6.64544571e-03  1.23295048e-02\n",
      "   4.54112841e-03  1.41732488e-02 -1.00201403e-03  1.61709208e-02\n",
      "   1.04835434e-02  1.76039059e-04  1.23709422e-02 -8.21558852e-03\n",
      "  -6.88615255e-03  3.64547805e-03  1.61179446e-03  2.79809954e-03\n",
      "   9.08206962e-03 -9.69118811e-03  1.19231227e-04  2.60391319e-03\n",
      "   4.07033274e-03 -2.64196773e-03 -2.76831933e-03 -7.21886242e-03\n",
      "   5.01030532e-04 -3.11430497e-03 -1.01330187e-02  1.08001232e-02\n",
      "   1.12484898e-02  8.09127174e-04  3.67395696e-03 -6.76418981e-03\n",
      "  -1.97379682e-02  5.82790328e-03 -4.32402454e-03 -1.45669933e-02]\n",
      " [ 5.85875241e-03  1.70222344e-03 -1.59702562e-02 -3.80317634e-03\n",
      "   8.59127380e-03  1.26997218e-03  7.19750393e-03  4.38431773e-04\n",
      "  -2.20539887e-03 -1.15826279e-02 -5.06130978e-03  3.21924174e-03\n",
      "   7.82972493e-04  5.41203329e-03 -1.07392427e-02 -1.60149671e-02\n",
      "   2.13815062e-03  4.45238920e-03 -1.13030737e-02  8.50827526e-03\n",
      "   3.55418236e-03  5.47871972e-03 -3.70314065e-03 -1.84088756e-04\n",
      "   3.04086437e-03 -2.90623193e-05 -2.95668487e-02 -1.48617756e-03\n",
      "   9.95276496e-03  1.97701724e-04  8.24782066e-03 -2.59808917e-03\n",
      "  -8.20133928e-03 -1.33922631e-02  3.93473590e-03  3.54012265e-03\n",
      "   9.44806845e-04  5.83883934e-03 -1.46591850e-02  4.52858134e-04\n",
      "   1.15636978e-02  1.09024905e-02 -2.96754250e-03  1.84208751e-02\n",
      "   8.17985740e-03  4.47139796e-03  1.16763329e-02  1.98423141e-03\n",
      "   2.14233436e-02 -2.16752477e-02  3.38423741e-03 -1.63365509e-02\n",
      "   3.00808344e-03 -6.05037343e-03  2.01057941e-02 -6.05007028e-03\n",
      "   7.22176535e-03  1.01936040e-02 -2.67684292e-02 -2.50982190e-03\n",
      "  -1.63576175e-02  2.33837054e-03 -8.96878913e-03 -1.45991994e-02]\n",
      " [-9.35395597e-04 -1.79928530e-03  1.47090340e-02 -1.56564228e-02\n",
      "  -2.02826448e-02  2.01247782e-02  1.00906063e-02  6.09314023e-03\n",
      "   3.76005284e-03 -2.47694948e-03  6.69257948e-03  2.61600129e-04\n",
      "  -6.27434393e-03 -1.14125200e-02 -6.12995960e-03  9.33852512e-03\n",
      "  -1.66254304e-02 -1.02839200e-02  1.18735211e-03 -9.57177859e-03\n",
      "   1.04023628e-02  1.55254379e-02 -4.11739980e-04 -1.29211089e-02\n",
      "  -1.54167553e-03  5.55816246e-03  1.40602058e-02  4.09493269e-03\n",
      "   7.24255049e-04 -4.21764422e-03 -1.38172004e-02  9.44817904e-03\n",
      "  -7.74321891e-03 -1.71655789e-02  1.10376009e-03  8.68804846e-03\n",
      "   3.07838735e-03 -1.18210388e-03 -1.59338731e-02 -4.49999468e-03\n",
      "  -2.11238605e-03  9.32721421e-03 -1.09196575e-02 -6.67537283e-03\n",
      "  -9.85810906e-03 -2.59115896e-03  1.30040152e-02 -4.29914100e-03\n",
      "  -1.81342009e-02  3.56658176e-03 -2.33340356e-03  4.12183395e-03\n",
      "  -3.14997975e-03 -3.00685642e-03 -3.39561841e-04 -2.02928740e-03\n",
      "  -1.96738006e-03  2.21163221e-03  1.17512271e-02  7.95272831e-03\n",
      "   1.59992203e-02 -5.19673387e-03  5.75868820e-04  1.26827555e-02]\n",
      " [-2.60444032e-03 -4.51702997e-03  1.46502592e-02  6.17686519e-03\n",
      "   8.75683781e-03 -1.61798093e-02  1.40028084e-02  5.22502232e-03\n",
      "   2.72609834e-02  1.28461765e-02  1.43515533e-02  6.01165602e-03\n",
      "  -3.32220155e-03  6.36003073e-03  5.67551004e-03  7.86180701e-03\n",
      "  -5.83576330e-05  1.42730528e-03 -2.65416782e-03 -4.77295369e-03\n",
      "   9.36205871e-03 -8.95273592e-03 -2.19303253e-03 -9.60325450e-03\n",
      "  -1.12432009e-03  1.02804285e-02  9.40457918e-03  3.86545318e-03\n",
      "  -8.61212239e-03 -4.85576893e-04 -7.28766480e-03 -1.10003557e-02\n",
      "   2.54334137e-02 -1.60218729e-03 -5.63642615e-03  9.58976336e-03\n",
      "   1.45247020e-02 -6.76094601e-03  1.58721721e-03 -7.56857265e-03\n",
      "   4.58491314e-03 -1.01813842e-02 -8.52405466e-03 -1.02169914e-02\n",
      "   1.34363957e-03 -4.44913894e-04  1.91496555e-02 -8.74423329e-03\n",
      "  -1.14600342e-02 -7.68187502e-03  1.19170370e-02  2.74094287e-03\n",
      "  -7.30314758e-03  6.10786583e-03 -3.23261437e-03 -9.37406253e-03\n",
      "   4.35130810e-03  9.14207939e-03  8.79824534e-03 -4.20075003e-03\n",
      "   2.88351555e-04 -1.36139290e-02 -3.32226465e-03  1.19643286e-02]\n",
      " [ 2.16329959e-03 -2.02958230e-02  3.04733147e-03 -7.85077270e-03\n",
      "  -9.76342149e-03  1.03905778e-02 -1.28544848e-02  2.10831519e-02\n",
      "  -7.71599414e-04 -1.33048336e-03 -5.40683931e-03  1.02339480e-02\n",
      "   7.33587891e-04 -1.19650960e-02 -6.28576614e-03 -1.38489539e-02\n",
      "   1.25114927e-02 -3.97728011e-03 -6.03220752e-03 -3.59654822e-03\n",
      "  -6.25717454e-04 -1.55053316e-02  6.80543482e-04 -1.83294043e-02\n",
      "  -4.34148172e-03  1.44886579e-02  3.08001623e-03  2.25637462e-02\n",
      "  -1.46104190e-02  3.38613032e-03  7.91872665e-03  3.14808404e-03\n",
      "  -1.35573070e-03 -8.20105709e-03 -1.53987324e-02  2.63261460e-02\n",
      "  -1.27436947e-02 -2.40310710e-02 -1.19669552e-04 -5.82661363e-04\n",
      "  -3.22336261e-03  4.85593826e-03 -5.43137453e-03  1.21855410e-02\n",
      "  -3.68208368e-03  4.35017422e-03  1.26107354e-02  4.67587262e-03\n",
      "  -7.88377598e-03  5.66725014e-03 -1.79072283e-03  1.25733512e-02\n",
      "   4.04358748e-03 -8.72382335e-03 -3.05075501e-03  4.04821988e-03\n",
      "  -8.61386769e-03 -6.35088468e-03 -1.33737288e-02  2.04900908e-03\n",
      "   7.28511950e-03  1.27059985e-02  1.66993041e-03  8.27871263e-03]]\n",
      "b3: [ 0.01311477  0.01056809  0.02750178 -0.00232592  0.0046995   0.01742934\n",
      "  0.00762854 -0.00074124  0.00858766  0.02165046]\n"
     ]
    }
   ],
   "source": [
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "from pycuda.compiler import SourceModule\n",
    "import numpy as np\n",
    "\n",
    "# -------------------------------\n",
    "# Define network dimensions and training parameters.\n",
    "# -------------------------------\n",
    "batch_size   = 64    # samples per batch\n",
    "num_batches  = 10    # number of batches to process\n",
    "input_dim    = 784   # e.g. flattened MNIST images\n",
    "hidden1_dim  = 128\n",
    "hidden2_dim  = 64\n",
    "output_dim   = 10    # number of classes\n",
    "learning_rate = 0.01\n",
    "\n",
    "# -------------------------------\n",
    "# CUDA Kernels in CUDA C\n",
    "# -------------------------------\n",
    "kernel_code = r\"\"\"\n",
    "extern \"C\" {\n",
    "\n",
    "//\n",
    "// Kernel: trainStep\n",
    "// Each thread processes one sample: it computes the forward pass,\n",
    "// computes the backpropagated gradients, and writes per-sample gradients\n",
    "// into gradient buffers.\n",
    "//\n",
    "__global__ void trainStep(\n",
    "    const float *input, const float *labels,\n",
    "    float *hidden1, float *hidden2, float *output,\n",
    "    const float *weights1, const float *bias1,\n",
    "    const float *weights2, const float *bias2,\n",
    "    const float *weights3, const float *bias3,\n",
    "    float *grad_dW1, float *grad_db1,\n",
    "    float *grad_dW2, float *grad_db2,\n",
    "    float *grad_dW3, float *grad_db3,\n",
    "    int batch_size, int input_dim, int hidden1_dim, int hidden2_dim, int output_dim)\n",
    "{\n",
    "    int sample = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (sample >= batch_size) return;\n",
    "\n",
    "    // Pointers to this sample's data.\n",
    "    const float *x = input + sample * input_dim;\n",
    "    const float *label = labels + sample * output_dim;\n",
    "    float *h1 = hidden1 + sample * hidden1_dim;\n",
    "    float *h2 = hidden2 + sample * hidden2_dim;\n",
    "    float *out = output + sample * output_dim;\n",
    "\n",
    "    // Pointers to gradient buffers for this sample.\n",
    "    float *gradW1 = grad_dW1 + sample * hidden1_dim * input_dim;\n",
    "    float *gradb1 = grad_db1 + sample * hidden1_dim;\n",
    "    float *gradW2 = grad_dW2 + sample * hidden2_dim * hidden1_dim;\n",
    "    float *gradb2 = grad_db2 + sample * hidden2_dim;\n",
    "    float *gradW3 = grad_dW3 + sample * output_dim * hidden2_dim;\n",
    "    float *gradb3 = grad_db3 + sample * output_dim;\n",
    "\n",
    "    // --------------------\n",
    "    // Forward Pass\n",
    "    // --------------------\n",
    "    // Layer 1: h1 = ReLU(W1*x + bias1)\n",
    "    for (int i = 0; i < hidden1_dim; i++) {\n",
    "        float sum = bias1[i];\n",
    "        for (int j = 0; j < input_dim; j++) {\n",
    "            sum += weights1[i * input_dim + j] * x[j];\n",
    "        }\n",
    "        h1[i] = (sum > 0) ? sum : 0;\n",
    "    }\n",
    "\n",
    "    // Layer 2: h2 = ReLU(W2*h1 + bias2)\n",
    "    for (int i = 0; i < hidden2_dim; i++) {\n",
    "        float sum = bias2[i];\n",
    "        for (int j = 0; j < hidden1_dim; j++) {\n",
    "            sum += weights2[i * hidden1_dim + j] * h1[j];\n",
    "        }\n",
    "        h2[i] = (sum > 0) ? sum : 0;\n",
    "    }\n",
    "\n",
    "    // Output layer: out = W3*h2 + bias3  (logits)\n",
    "    for (int i = 0; i < output_dim; i++) {\n",
    "        float sum = bias3[i];\n",
    "        for (int j = 0; j < hidden2_dim; j++) {\n",
    "            sum += weights3[i * hidden2_dim + j] * h2[j];\n",
    "        }\n",
    "        out[i] = sum;\n",
    "    }\n",
    "\n",
    "    // --------------------\n",
    "    // Backpropagation\n",
    "    // --------------------\n",
    "    // Compute delta3 = out - label.\n",
    "    float delta3[16]; // output_dim is 10 (16 is a safe upper bound)\n",
    "    for (int i = 0; i < output_dim; i++) {\n",
    "        delta3[i] = out[i] - label[i];\n",
    "        gradb3[i] = delta3[i];\n",
    "    }\n",
    "    // Gradients for weights3: outer product delta3 * h2.\n",
    "    for (int i = 0; i < output_dim; i++) {\n",
    "        for (int j = 0; j < hidden2_dim; j++) {\n",
    "            gradW3[i * hidden2_dim + j] = delta3[i] * h2[j];\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // Backprop for layer 2: delta2 = (W3^T * delta3) .* ReLU'(h2)\n",
    "    float delta2[64]; // hidden2_dim is 64.\n",
    "    for (int i = 0; i < hidden2_dim; i++) {\n",
    "        float sum = 0;\n",
    "        for (int j = 0; j < output_dim; j++) {\n",
    "            sum += weights3[j * hidden2_dim + i] * delta3[j];\n",
    "        }\n",
    "        delta2[i] = (h2[i] > 0) ? sum : 0;\n",
    "        gradb2[i] = delta2[i];\n",
    "    }\n",
    "    // Gradients for weights2: outer product delta2 * h1.\n",
    "    for (int i = 0; i < hidden2_dim; i++) {\n",
    "        for (int j = 0; j < hidden1_dim; j++) {\n",
    "            gradW2[i * hidden1_dim + j] = delta2[i] * h1[j];\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // Backprop for layer 1: delta1 = (W2^T * delta2) .* ReLU'(h1)\n",
    "    float delta1[128]; // hidden1_dim is 128.\n",
    "    for (int i = 0; i < hidden1_dim; i++) {\n",
    "        float sum = 0;\n",
    "        for (int j = 0; j < hidden2_dim; j++) {\n",
    "            sum += weights2[j * hidden1_dim + i] * delta2[j];\n",
    "        }\n",
    "        delta1[i] = (h1[i] > 0) ? sum : 0;\n",
    "        gradb1[i] = delta1[i];\n",
    "    }\n",
    "    // Gradients for weights1: outer product delta1 * x.\n",
    "    for (int i = 0; i < hidden1_dim; i++) {\n",
    "        for (int j = 0; j < input_dim; j++) {\n",
    "            gradW1[i * input_dim + j] = delta1[i] * x[j];\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "//\n",
    "// Kernel: updateWeights\n",
    "// For a given parameter array (or bias vector), this kernel sums the gradient\n",
    "// values over the batch, averages them, and then updates the parameter.\n",
    "//\n",
    "__global__ void updateWeights(\n",
    "    float *param, const float *grad_buffer,\n",
    "    int count, int batch_size, float learning_rate)\n",
    "{\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx >= count) return;\n",
    "    float sum = 0;\n",
    "    for (int i = 0; i < batch_size; i++) {\n",
    "        sum += grad_buffer[i * count + idx];\n",
    "    }\n",
    "    float grad_avg = sum / batch_size;\n",
    "\n",
    "    // Optionally clip the gradient (uncomment the following lines if desired):\n",
    "    // if (grad_avg > 1.0f) grad_avg = 1.0f;\n",
    "    // if (grad_avg < -1.0f) grad_avg = -1.0f;\n",
    "\n",
    "    param[idx] -= learning_rate * grad_avg;\n",
    "}\n",
    "\n",
    "} // extern \"C\"\n",
    "\"\"\"\n",
    "\n",
    "# -------------------------------\n",
    "# Compile the CUDA kernels.\n",
    "# -------------------------------\n",
    "mod = SourceModule(kernel_code)\n",
    "trainStep = mod.get_function(\"trainStep\")\n",
    "updateWeights = mod.get_function(\"updateWeights\")\n",
    "\n",
    "# -------------------------------\n",
    "# Utility function to generate a random batch.\n",
    "# -------------------------------\n",
    "def generate_batch_data(batch_size, input_dim, output_dim):\n",
    "    x = np.random.randn(batch_size, input_dim).astype(np.float32)\n",
    "    labels = np.zeros((batch_size, output_dim), dtype=np.float32)\n",
    "    for i in range(batch_size):\n",
    "        lbl = np.random.randint(0, output_dim)\n",
    "        labels[i, lbl] = 1.0\n",
    "    return x, labels\n",
    "\n",
    "# -------------------------------\n",
    "# Initialize weight matrices and biases with small values.\n",
    "# -------------------------------\n",
    "scale = 0.01\n",
    "weights1 = (np.random.randn(hidden1_dim, input_dim) * scale).astype(np.float32)\n",
    "bias1    = (np.random.randn(hidden1_dim) * scale).astype(np.float32)\n",
    "weights2 = (np.random.randn(hidden2_dim, hidden1_dim) * scale).astype(np.float32)\n",
    "bias2    = (np.random.randn(hidden2_dim) * scale).astype(np.float32)\n",
    "weights3 = (np.random.randn(output_dim, hidden2_dim) * scale).astype(np.float32)\n",
    "bias3    = (np.random.randn(output_dim) * scale).astype(np.float32)\n",
    "\n",
    "# -------------------------------\n",
    "# Allocate GPU memory for weights and biases.\n",
    "# -------------------------------\n",
    "weights1_gpu = cuda.mem_alloc(weights1.nbytes)\n",
    "bias1_gpu    = cuda.mem_alloc(bias1.nbytes)\n",
    "weights2_gpu = cuda.mem_alloc(weights2.nbytes)\n",
    "bias2_gpu    = cuda.mem_alloc(bias2.nbytes)\n",
    "weights3_gpu = cuda.mem_alloc(weights3.nbytes)\n",
    "bias3_gpu    = cuda.mem_alloc(bias3.nbytes)\n",
    "cuda.memcpy_htod(weights1_gpu, weights1)\n",
    "cuda.memcpy_htod(bias1_gpu, bias1)\n",
    "cuda.memcpy_htod(weights2_gpu, weights2)\n",
    "cuda.memcpy_htod(bias2_gpu, bias2)\n",
    "cuda.memcpy_htod(weights3_gpu, weights3)\n",
    "cuda.memcpy_htod(bias3_gpu, bias3)\n",
    "\n",
    "# -------------------------------\n",
    "# Allocate buffers for activations and outputs.\n",
    "# -------------------------------\n",
    "hidden1_gpu = cuda.mem_alloc(batch_size * hidden1_dim * np.float32().nbytes)\n",
    "hidden2_gpu = cuda.mem_alloc(batch_size * hidden2_dim * np.float32().nbytes)\n",
    "output_gpu  = cuda.mem_alloc(batch_size * output_dim * np.float32().nbytes)\n",
    "\n",
    "# -------------------------------\n",
    "# Allocate buffers for input and labels (reused per batch).\n",
    "# -------------------------------\n",
    "input_gpu   = cuda.mem_alloc(batch_size * input_dim * np.float32().nbytes)\n",
    "labels_gpu  = cuda.mem_alloc(batch_size * output_dim * np.float32().nbytes)\n",
    "\n",
    "# -------------------------------\n",
    "# Allocate gradient buffers.\n",
    "# For simplicity we use one set (each of size: batch_size * (parameter size)).\n",
    "# -------------------------------\n",
    "grad_dW1_gpu = cuda.mem_alloc(batch_size * hidden1_dim * input_dim * np.float32().nbytes)\n",
    "grad_db1_gpu = cuda.mem_alloc(batch_size * hidden1_dim * np.float32().nbytes)\n",
    "grad_dW2_gpu = cuda.mem_alloc(batch_size * hidden2_dim * hidden1_dim * np.float32().nbytes)\n",
    "grad_db2_gpu = cuda.mem_alloc(batch_size * hidden2_dim * np.float32().nbytes)\n",
    "grad_dW3_gpu = cuda.mem_alloc(batch_size * output_dim * hidden2_dim * np.float32().nbytes)\n",
    "grad_db3_gpu = cuda.mem_alloc(batch_size * output_dim * np.float32().nbytes)\n",
    "\n",
    "# -------------------------------\n",
    "# Define grid and block dimensions for trainStep.\n",
    "# Each thread handles one sample.\n",
    "# -------------------------------\n",
    "threads_per_block = 256\n",
    "grid_dim = ((batch_size + threads_per_block - 1) // threads_per_block, 1, 1)\n",
    "block_dim = (threads_per_block, 1, 1)\n",
    "\n",
    "# -------------------------------\n",
    "# Training loop.\n",
    "# For each batch: generate data, run forward/backprop kernel, then update weights.\n",
    "# -------------------------------\n",
    "for batch in range(num_batches):\n",
    "    # Generate batch data.\n",
    "    x_batch, labels_batch = generate_batch_data(batch_size, input_dim, output_dim)\n",
    "    cuda.memcpy_htod(input_gpu, x_batch)\n",
    "    cuda.memcpy_htod(labels_gpu, labels_batch)\n",
    "    \n",
    "    # Launch the forward/backprop kernel.\n",
    "    trainStep(\n",
    "        input_gpu, labels_gpu,\n",
    "        hidden1_gpu, hidden2_gpu, output_gpu,\n",
    "        weights1_gpu, bias1_gpu,\n",
    "        weights2_gpu, bias2_gpu,\n",
    "        weights3_gpu, bias3_gpu,\n",
    "        grad_dW1_gpu, grad_db1_gpu,\n",
    "        grad_dW2_gpu, grad_db2_gpu,\n",
    "        grad_dW3_gpu, grad_db3_gpu,\n",
    "        np.int32(batch_size), np.int32(input_dim), np.int32(hidden1_dim),\n",
    "        np.int32(hidden2_dim), np.int32(output_dim),\n",
    "        block=block_dim, grid=grid_dim\n",
    "    )\n",
    "    \n",
    "    # For each parameter array, run the update kernel.\n",
    "    # updateWeights will sum over the batch dimension and update the parameter.\n",
    "    threads = 256\n",
    "\n",
    "    # Update weights1.\n",
    "    count_w1 = np.int32(hidden1_dim * input_dim)\n",
    "    grid = ((int(count_w1) + threads - 1) // threads, 1, 1)\n",
    "    updateWeights(\n",
    "        weights1_gpu, grad_dW1_gpu,\n",
    "        count_w1, np.int32(batch_size), np.float32(learning_rate),\n",
    "        block=(threads, 1, 1), grid=grid\n",
    "    )\n",
    "    # Update bias1.\n",
    "    count_b1 = np.int32(hidden1_dim)\n",
    "    grid = ((int(count_b1) + threads - 1) // threads, 1, 1)\n",
    "    updateWeights(\n",
    "        bias1_gpu, grad_db1_gpu,\n",
    "        count_b1, np.int32(batch_size), np.float32(learning_rate),\n",
    "        block=(threads, 1, 1), grid=grid\n",
    "    )\n",
    "    # Update weights2.\n",
    "    count_w2 = np.int32(hidden2_dim * hidden1_dim)\n",
    "    grid = ((int(count_w2) + threads - 1) // threads, 1, 1)\n",
    "    updateWeights(\n",
    "        weights2_gpu, grad_dW2_gpu,\n",
    "        count_w2, np.int32(batch_size), np.float32(learning_rate),\n",
    "        block=(threads, 1, 1), grid=grid\n",
    "    )\n",
    "    # Update bias2.\n",
    "    count_b2 = np.int32(hidden2_dim)\n",
    "    grid = ((int(count_b2) + threads - 1) // threads, 1, 1)\n",
    "    updateWeights(\n",
    "        bias2_gpu, grad_db2_gpu,\n",
    "        count_b2, np.int32(batch_size), np.float32(learning_rate),\n",
    "        block=(threads, 1, 1), grid=grid\n",
    "    )\n",
    "    # Update weights3.\n",
    "    count_w3 = np.int32(output_dim * hidden2_dim)\n",
    "    grid = ((int(count_w3) + threads - 1) // threads, 1, 1)\n",
    "    updateWeights(\n",
    "        weights3_gpu, grad_dW3_gpu,\n",
    "        count_w3, np.int32(batch_size), np.float32(learning_rate),\n",
    "        block=(threads, 1, 1), grid=grid\n",
    "    )\n",
    "    # Update bias3.\n",
    "    count_b3 = np.int32(output_dim)\n",
    "    grid = ((int(count_b3) + threads - 1) // threads, 1, 1)\n",
    "    updateWeights(\n",
    "        bias3_gpu, grad_db3_gpu,\n",
    "        count_b3, np.int32(batch_size), np.float32(learning_rate),\n",
    "        block=(threads, 1, 1), grid=grid\n",
    "    )\n",
    "    \n",
    "    print(\"Completed batch\", batch+1)\n",
    "\n",
    "# -------------------------------\n",
    "# Copy updated weights back to host for verification.\n",
    "# -------------------------------\n",
    "cuda.memcpy_dtoh(weights1, weights1_gpu)\n",
    "cuda.memcpy_dtoh(bias1, bias1_gpu)\n",
    "cuda.memcpy_dtoh(weights2, weights2_gpu)\n",
    "cuda.memcpy_dtoh(bias2, bias2_gpu)\n",
    "cuda.memcpy_dtoh(weights3, weights3_gpu)\n",
    "cuda.memcpy_dtoh(bias3, bias3_gpu)\n",
    "\n",
    "print(\"Training complete.\")\n",
    "print(\"Updated weights and biases:\")\n",
    "print(\"W1:\", weights1)\n",
    "print(\"b1:\", bias1)\n",
    "print(\"W2:\", weights2)\n",
    "print(\"b2:\", bias2)\n",
    "print(\"W3:\", weights3)\n",
    "print(\"b3:\", bias3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This worked! So we start training now! We'll train until it reaches 60% accuracy on the data. Yes I know this isn't the best practice because we need to incorporate validation data, but the point of this exercise is to write it in cuda not to have a full machine learning work flow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting PyCUDA Training (Two-Layer Network)...\n",
      "PyCUDA: Batch 100, Test Accuracy: 8.09%\n",
      "PyCUDA: Batch 200, Test Accuracy: 11.60%\n",
      "PyCUDA: Batch 300, Test Accuracy: 14.89%\n",
      "PyCUDA: Batch 400, Test Accuracy: 17.08%\n",
      "PyCUDA: Batch 500, Test Accuracy: 19.42%\n",
      "PyCUDA: Batch 600, Test Accuracy: 22.38%\n",
      "PyCUDA: Batch 700, Test Accuracy: 27.20%\n",
      "PyCUDA: Batch 800, Test Accuracy: 34.27%\n",
      "PyCUDA: Batch 900, Test Accuracy: 41.09%\n",
      "PyCUDA: Batch 1000, Test Accuracy: 46.24%\n",
      "PyCUDA: Batch 1100, Test Accuracy: 48.37%\n",
      "PyCUDA: Batch 1200, Test Accuracy: 51.23%\n",
      "PyCUDA: Batch 1300, Test Accuracy: 53.20%\n",
      "PyCUDA: Batch 1400, Test Accuracy: 55.29%\n",
      "PyCUDA: Batch 1500, Test Accuracy: 57.03%\n",
      "PyCUDA: Batch 1600, Test Accuracy: 58.38%\n",
      "PyCUDA: Batch 1700, Test Accuracy: 60.15%\n",
      "\n",
      "PyCUDA Training Finished in 48.1470 seconds over 1700 batches.\n",
      "PyCUDA Test Accuracy: 60.15%\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "from pycuda.compiler import SourceModule\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "############################################\n",
    "# Settings and Data Loading (Common to both)\n",
    "############################################\n",
    "batch_size   = 64\n",
    "input_dim    = 784   # MNIST: 28x28 flattened\n",
    "hidden_dim   = 128   # one hidden layer\n",
    "output_dim   = 10\n",
    "learning_rate = 0.001   # try a lower learning rate for stability\n",
    "max_batches = 5000      # maximum batches to run\n",
    "scale = 0.01            # weight initialization scale\n",
    "\n",
    "# Download and prepare MNIST dataset.\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: x.view(-1))])\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset  = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Helper: Convert integer labels to one-hot vectors.\n",
    "def one_hot(labels, num_classes=10):\n",
    "    one_hot_labels = np.zeros((labels.shape[0], num_classes), dtype=np.float32)\n",
    "    one_hot_labels[np.arange(labels.shape[0]), labels] = 1.0\n",
    "    return one_hot_labels\n",
    "\n",
    "# Simple NumPy forward pass (for evaluation) for a two-layer network.\n",
    "def forward_numpy(x, weights, bias, weights_out, bias_out):\n",
    "    # hidden layer: ReLU(W*x + b)\n",
    "    h = np.maximum(0, np.dot(x, weights.T) + bias)\n",
    "    # output layer: linear activation\n",
    "    out = np.dot(h, weights_out.T) + bias_out\n",
    "    return out\n",
    "\n",
    "# Evaluate accuracy using NumPy forward pass.\n",
    "def evaluate_model(weights, bias, weights_out, bias_out):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        x = images.numpy()  # shape: [batch_size, 784]\n",
    "        out = forward_numpy(x, weights, bias, weights_out, bias_out)\n",
    "        preds = np.argmax(out, axis=1)\n",
    "        total += labels.size(0)\n",
    "        correct += (preds == labels.numpy()).sum()\n",
    "    return correct / total\n",
    "\n",
    "############################################\n",
    "# 1. PyCUDA Implementation (Two-Layer Network)\n",
    "############################################\n",
    "cuda_kernel_code = r\"\"\"\n",
    "extern \"C\" {\n",
    "\n",
    "// Kernel: trainStep\n",
    "// Each thread processes one sample. It performs the forward pass,\n",
    "// computes backpropagated gradients using squared-error loss, and writes\n",
    "// per-sample gradients into separate gradient buffers.\n",
    "__global__ void trainStep(\n",
    "    const float *input, const float *labels,\n",
    "    float *hidden, float *output,\n",
    "    const float *weights, const float *bias,\n",
    "    const float *weights_out, const float *bias_out,\n",
    "    float *grad_dW, float *grad_db,\n",
    "    float *grad_dW_out, float *grad_db_out,\n",
    "    int batch_size, int input_dim, int hidden_dim, int output_dim)\n",
    "{\n",
    "    int sample = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if(sample >= batch_size) return;\n",
    "\n",
    "    // Pointers for this sample.\n",
    "    const float *x = input + sample * input_dim;\n",
    "    const float *label = labels + sample * output_dim;\n",
    "    float *h = hidden + sample * hidden_dim;\n",
    "    float *out = output + sample * output_dim;\n",
    "\n",
    "    // Pointers to per-sample gradient buffers.\n",
    "    float *gradW = grad_dW + sample * hidden_dim * input_dim;\n",
    "    float *gradb = grad_db + sample * hidden_dim;\n",
    "    float *gradW_out = grad_dW_out + sample * output_dim * hidden_dim;\n",
    "    float *gradb_out = grad_db_out + sample * output_dim;\n",
    "\n",
    "    // Forward pass: Compute hidden layer activations.\n",
    "    for (int i = 0; i < hidden_dim; i++) {\n",
    "        float sum = bias[i];\n",
    "        for (int j = 0; j < input_dim; j++) {\n",
    "            sum += weights[i * input_dim + j] * x[j];\n",
    "        }\n",
    "        h[i] = (sum > 0) ? sum : 0;  // ReLU activation.\n",
    "    }\n",
    "    // Output layer: Compute output.\n",
    "    for (int i = 0; i < output_dim; i++) {\n",
    "        float sum = bias_out[i];\n",
    "        for (int j = 0; j < hidden_dim; j++) {\n",
    "            sum += weights_out[i * hidden_dim + j] * h[j];\n",
    "        }\n",
    "        out[i] = sum;\n",
    "    }\n",
    "\n",
    "    // Backpropagation.\n",
    "    // Compute delta for output: delta_out = out - label.\n",
    "    float delta_out[16]; // assume output_dim <= 16.\n",
    "    for (int i = 0; i < output_dim; i++) {\n",
    "        delta_out[i] = out[i] - label[i];\n",
    "        gradb_out[i] = delta_out[i];\n",
    "    }\n",
    "    // Gradients for output weights: gradW_out = delta_out * h.\n",
    "    for (int i = 0; i < output_dim; i++) {\n",
    "        for (int j = 0; j < hidden_dim; j++) {\n",
    "            gradW_out[i * hidden_dim + j] = delta_out[i] * h[j];\n",
    "        }\n",
    "    }\n",
    "    // Backpropagate to hidden layer:\n",
    "    // delta_hidden = (weights_out^T * delta_out) * (h > 0 ? 1 : 0).\n",
    "    float delta_hidden[128]; // assume hidden_dim <= 128.\n",
    "    for (int i = 0; i < hidden_dim; i++) {\n",
    "        float sum = 0;\n",
    "        for (int j = 0; j < output_dim; j++) {\n",
    "            sum += weights_out[j * hidden_dim + i] * delta_out[j];\n",
    "        }\n",
    "        delta_hidden[i] = (h[i] > 0) ? sum : 0;\n",
    "        gradb[i] = delta_hidden[i];\n",
    "    }\n",
    "    // Gradients for input weights: gradW = delta_hidden * x.\n",
    "    for (int i = 0; i < hidden_dim; i++) {\n",
    "        for (int j = 0; j < input_dim; j++) {\n",
    "            gradW[i * input_dim + j] = delta_hidden[i] * x[j];\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "// Kernel: updateWeights\n",
    "// For a given parameter array and its per-sample gradient buffer,\n",
    "// sum the gradients over the batch, average them, and update the parameter.\n",
    "__global__ void updateWeights(\n",
    "    float *param, const float *grad_buffer,\n",
    "    int count, int batch_size, float learning_rate)\n",
    "{\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx >= count) return;\n",
    "    float sum = 0;\n",
    "    for (int i = 0; i < batch_size; i++) {\n",
    "        sum += grad_buffer[i * count + idx];\n",
    "    }\n",
    "    float grad_avg = sum / batch_size;\n",
    "    param[idx] -= learning_rate * grad_avg;\n",
    "}\n",
    "\n",
    "} // extern \"C\"\n",
    "\"\"\"\n",
    "\n",
    "# Compile the CUDA kernels.\n",
    "mod = SourceModule(cuda_kernel_code)\n",
    "trainStep = mod.get_function(\"trainStep\")\n",
    "updateWeights = mod.get_function(\"updateWeights\")\n",
    "\n",
    "# Initialize network parameters for PyCUDA.\n",
    "weights_cuda = (np.random.randn(hidden_dim, input_dim) * scale).astype(np.float32)\n",
    "bias_cuda    = (np.random.randn(hidden_dim) * scale).astype(np.float32)\n",
    "weights_out_cuda = (np.random.randn(output_dim, hidden_dim) * scale).astype(np.float32)\n",
    "bias_out_cuda    = (np.random.randn(output_dim) * scale).astype(np.float32)\n",
    "\n",
    "# Allocate GPU memory for parameters.\n",
    "weights_gpu = cuda.mem_alloc(weights_cuda.nbytes)\n",
    "bias_gpu    = cuda.mem_alloc(bias_cuda.nbytes)\n",
    "weights_out_gpu = cuda.mem_alloc(weights_out_cuda.nbytes)\n",
    "bias_out_gpu    = cuda.mem_alloc(bias_out_cuda.nbytes)\n",
    "cuda.memcpy_htod(weights_gpu, weights_cuda)\n",
    "cuda.memcpy_htod(bias_gpu, bias_cuda)\n",
    "cuda.memcpy_htod(weights_out_gpu, weights_out_cuda)\n",
    "cuda.memcpy_htod(bias_out_gpu, bias_out_cuda)\n",
    "\n",
    "# Allocate GPU memory for activations, outputs, inputs, labels, and gradients.\n",
    "hidden_gpu   = cuda.mem_alloc(batch_size * hidden_dim * np.float32().nbytes)\n",
    "output_gpu   = cuda.mem_alloc(batch_size * output_dim * np.float32().nbytes)\n",
    "input_gpu    = cuda.mem_alloc(batch_size * input_dim * np.float32().nbytes)\n",
    "labels_gpu   = cuda.mem_alloc(batch_size * output_dim * np.float32().nbytes)\n",
    "grad_dW_gpu  = cuda.mem_alloc(batch_size * hidden_dim * input_dim * np.float32().nbytes)\n",
    "grad_db_gpu  = cuda.mem_alloc(batch_size * hidden_dim * np.float32().nbytes)\n",
    "grad_dW_out_gpu = cuda.mem_alloc(batch_size * output_dim * hidden_dim * np.float32().nbytes)\n",
    "grad_db_out_gpu = cuda.mem_alloc(batch_size * output_dim * np.float32().nbytes)\n",
    "\n",
    "# Define grid and block dimensions.\n",
    "threads_per_block = 256\n",
    "grid_dim = ((batch_size + threads_per_block - 1) // threads_per_block, 1, 1)\n",
    "block_dim = (threads_per_block, 1, 1)\n",
    "\n",
    "# PyCUDA Training Loop: Train until test accuracy exceeds 60% or max_batches reached.\n",
    "print(\"Starting PyCUDA Training (Two-Layer Network)...\")\n",
    "cuda_start = time.time()\n",
    "batch_count_cuda = 0\n",
    "accuracy_cuda = 0\n",
    "train_iter = iter(train_loader)\n",
    "while accuracy_cuda < 0.6 and batch_count_cuda < max_batches:\n",
    "    try:\n",
    "        images, labels = next(train_iter)\n",
    "    except StopIteration:\n",
    "        train_iter = iter(train_loader)\n",
    "        images, labels = next(train_iter)\n",
    "    x_batch = images.numpy()                     # shape: [batch_size, 784]\n",
    "    y_batch = one_hot(labels.numpy(), output_dim)  # one-hot: [batch_size, 10]\n",
    "    cuda.memcpy_htod(input_gpu, x_batch)\n",
    "    cuda.memcpy_htod(labels_gpu, y_batch)\n",
    "    # Launch trainStep kernel.\n",
    "    trainStep(\n",
    "        input_gpu, labels_gpu,\n",
    "        hidden_gpu, output_gpu,\n",
    "        weights_gpu, bias_gpu,\n",
    "        weights_out_gpu, bias_out_gpu,\n",
    "        grad_dW_gpu, grad_db_gpu,\n",
    "        grad_dW_out_gpu, grad_db_out_gpu,\n",
    "        np.int32(batch_size), np.int32(input_dim), np.int32(hidden_dim), np.int32(output_dim),\n",
    "        block=block_dim, grid=grid_dim\n",
    "    )\n",
    "    # Launch updateWeights kernel for each parameter.\n",
    "    threads = 256\n",
    "\n",
    "    # Update input-to-hidden weights.\n",
    "    count_w = np.int32(hidden_dim * input_dim)\n",
    "    grid_update = ((int(count_w) + threads - 1) // threads, 1, 1)\n",
    "    updateWeights(\n",
    "        weights_gpu, grad_dW_gpu,\n",
    "        count_w, np.int32(batch_size), np.float32(learning_rate),\n",
    "        block=(threads,1,1), grid=grid_update\n",
    "    )\n",
    "    # Update hidden biases.\n",
    "    count_b = np.int32(hidden_dim)\n",
    "    grid_update = ((int(count_b) + threads - 1) // threads, 1, 1)\n",
    "    updateWeights(\n",
    "        bias_gpu, grad_db_gpu,\n",
    "        count_b, np.int32(batch_size), np.float32(learning_rate),\n",
    "        block=(threads,1,1), grid=grid_update\n",
    "    )\n",
    "    # Update hidden-to-output weights.\n",
    "    count_w_out = np.int32(output_dim * hidden_dim)\n",
    "    grid_update = ((int(count_w_out) + threads - 1) // threads, 1, 1)\n",
    "    updateWeights(\n",
    "        weights_out_gpu, grad_dW_out_gpu,\n",
    "        count_w_out, np.int32(batch_size), np.float32(learning_rate),\n",
    "        block=(threads,1,1), grid=grid_update\n",
    "    )\n",
    "    # Update output biases.\n",
    "    count_b_out = np.int32(output_dim)\n",
    "    grid_update = ((int(count_b_out) + threads - 1) // threads, 1, 1)\n",
    "    updateWeights(\n",
    "        bias_out_gpu, grad_db_out_gpu,\n",
    "        count_b_out, np.int32(batch_size), np.float32(learning_rate),\n",
    "        block=(threads,1,1), grid=grid_update\n",
    "    )\n",
    "    batch_count_cuda += 1\n",
    "\n",
    "    # Every 100 batches, evaluate test accuracy.\n",
    "    if batch_count_cuda % 100 == 0:\n",
    "        # Copy parameters back to host.\n",
    "        cuda.memcpy_dtoh(weights_cuda, weights_gpu)\n",
    "        cuda.memcpy_dtoh(bias_cuda, bias_gpu)\n",
    "        cuda.memcpy_dtoh(weights_out_cuda, weights_out_gpu)\n",
    "        cuda.memcpy_dtoh(bias_out_cuda, bias_out_gpu)\n",
    "        accuracy_cuda = evaluate_model(weights_cuda, bias_cuda, weights_out_cuda, bias_out_cuda)\n",
    "        print(f\"PyCUDA: Batch {batch_count_cuda}, Test Accuracy: {accuracy_cuda*100:.2f}%\")\n",
    "cuda_end = time.time()\n",
    "cuda_time = cuda_end - cuda_start\n",
    "print(f\"\\nPyCUDA Training Finished in {cuda_time:.4f} seconds over {batch_count_cuda} batches.\")\n",
    "print(f\"PyCUDA Test Accuracy: {accuracy_cuda*100:.2f}%\")\n",
    "\n",
    "############################################\n",
    "# End of PyCUDA Section\n",
    "############################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compare the code with a the same implementation in Pytorch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting PyCUDA Training (Two-Layer Network)...\n",
      "PyCUDA: Batch 100, Test Accuracy: 13.01%\n",
      "PyCUDA: Batch 200, Test Accuracy: 14.63%\n",
      "PyCUDA: Batch 300, Test Accuracy: 17.43%\n",
      "PyCUDA: Batch 400, Test Accuracy: 22.13%\n",
      "PyCUDA: Batch 500, Test Accuracy: 27.75%\n",
      "PyCUDA: Batch 600, Test Accuracy: 35.57%\n",
      "PyCUDA: Batch 700, Test Accuracy: 42.39%\n",
      "PyCUDA: Batch 800, Test Accuracy: 48.74%\n",
      "PyCUDA: Batch 900, Test Accuracy: 51.74%\n",
      "PyCUDA: Batch 1000, Test Accuracy: 55.54%\n",
      "PyCUDA: Batch 1100, Test Accuracy: 58.98%\n",
      "PyCUDA: Batch 1200, Test Accuracy: 61.64%\n",
      "\n",
      "PyCUDA Training Finished in 33.4456 seconds over 1200 batches.\n",
      "PyCUDA Test Accuracy: 61.64%\n",
      "\n",
      "Starting PyTorch Training...\n",
      "PyTorch: Batch 100, Test Accuracy: 12.18%\n",
      "PyTorch: Batch 200, Test Accuracy: 13.38%\n",
      "PyTorch: Batch 300, Test Accuracy: 15.13%\n",
      "PyTorch: Batch 400, Test Accuracy: 17.71%\n",
      "PyTorch: Batch 500, Test Accuracy: 19.94%\n",
      "PyTorch: Batch 600, Test Accuracy: 22.34%\n",
      "PyTorch: Batch 700, Test Accuracy: 24.76%\n",
      "PyTorch: Batch 800, Test Accuracy: 26.95%\n",
      "PyTorch: Batch 900, Test Accuracy: 28.78%\n",
      "PyTorch: Batch 1000, Test Accuracy: 30.73%\n",
      "PyTorch: Batch 1100, Test Accuracy: 32.63%\n",
      "PyTorch: Batch 1200, Test Accuracy: 34.24%\n",
      "PyTorch: Batch 1300, Test Accuracy: 36.04%\n",
      "PyTorch: Batch 1400, Test Accuracy: 37.54%\n",
      "PyTorch: Batch 1500, Test Accuracy: 39.01%\n",
      "PyTorch: Batch 1600, Test Accuracy: 40.12%\n",
      "PyTorch: Batch 1700, Test Accuracy: 41.34%\n",
      "PyTorch: Batch 1800, Test Accuracy: 42.26%\n",
      "PyTorch: Batch 1900, Test Accuracy: 43.50%\n",
      "PyTorch: Batch 2000, Test Accuracy: 44.63%\n",
      "PyTorch: Batch 2100, Test Accuracy: 45.60%\n",
      "PyTorch: Batch 2200, Test Accuracy: 46.51%\n",
      "PyTorch: Batch 2300, Test Accuracy: 47.60%\n",
      "PyTorch: Batch 2400, Test Accuracy: 48.71%\n",
      "PyTorch: Batch 2500, Test Accuracy: 49.58%\n",
      "PyTorch: Batch 2600, Test Accuracy: 50.21%\n",
      "PyTorch: Batch 2700, Test Accuracy: 51.09%\n",
      "PyTorch: Batch 2800, Test Accuracy: 52.00%\n",
      "PyTorch: Batch 2900, Test Accuracy: 52.76%\n",
      "PyTorch: Batch 3000, Test Accuracy: 53.79%\n",
      "PyTorch: Batch 3100, Test Accuracy: 54.36%\n",
      "PyTorch: Batch 3200, Test Accuracy: 54.83%\n",
      "PyTorch: Batch 3300, Test Accuracy: 55.43%\n",
      "PyTorch: Batch 3400, Test Accuracy: 56.18%\n",
      "PyTorch: Batch 3500, Test Accuracy: 56.74%\n",
      "PyTorch: Batch 3600, Test Accuracy: 57.20%\n",
      "PyTorch: Batch 3700, Test Accuracy: 57.79%\n",
      "PyTorch: Batch 3800, Test Accuracy: 58.38%\n",
      "PyTorch: Batch 3900, Test Accuracy: 58.91%\n",
      "PyTorch: Batch 4000, Test Accuracy: 59.41%\n",
      "PyTorch: Batch 4100, Test Accuracy: 59.87%\n",
      "PyTorch: Batch 4200, Test Accuracy: 60.33%\n",
      "\n",
      "PyTorch Training Finished in 67.6265 seconds over 4200 batches.\n",
      "PyTorch Test Accuracy: 60.33%\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "from pycuda.compiler import SourceModule\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "###############################\n",
    "# Common Settings and Data Loading\n",
    "###############################\n",
    "batch_size    = 64\n",
    "input_dim     = 784   # 28x28 flattened\n",
    "hidden_dim    = 128\n",
    "output_dim    = 10\n",
    "learning_rate = 0.001\n",
    "max_batches   = 10000   # safety maximum\n",
    "scale         = 0.01    # weight initialization scale\n",
    "\n",
    "# Download MNIST dataset using torchvision.\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: x.view(-1))])\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset  = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "train_loader  = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader   = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Helper: convert integer labels to one-hot vectors.\n",
    "def one_hot(labels, num_classes=10):\n",
    "    one_hot_labels = np.zeros((labels.shape[0], num_classes), dtype=np.float32)\n",
    "    one_hot_labels[np.arange(labels.shape[0]), labels] = 1.0\n",
    "    return one_hot_labels\n",
    "\n",
    "# Simple NumPy forward pass for a two-layer network.\n",
    "def forward_numpy(x, weights, bias, weights_out, bias_out):\n",
    "    h = np.maximum(0, np.dot(x, weights.T) + bias)\n",
    "    out = np.dot(h, weights_out.T) + bias_out\n",
    "    return out\n",
    "\n",
    "# Evaluate test accuracy using NumPy.\n",
    "def evaluate_model(weights, bias, weights_out, bias_out):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        x = images.numpy()  # shape: [batch_size, 784]\n",
    "        out = forward_numpy(x, weights, bias, weights_out, bias_out)\n",
    "        preds = np.argmax(out, axis=1)\n",
    "        total += labels.size(0)\n",
    "        correct += (preds == labels.numpy()).sum()\n",
    "    return correct / total\n",
    "\n",
    "###############################\n",
    "# 1. PyCUDA Implementation\n",
    "###############################\n",
    "# Two-layer network CUDA kernels (using squared error loss on one-hot targets).\n",
    "cuda_kernel_code = r\"\"\"\n",
    "extern \"C\" {\n",
    "\n",
    "__global__ void trainStep(\n",
    "    const float *input, const float *labels,\n",
    "    float *hidden, float *output,\n",
    "    const float *weights, const float *bias,\n",
    "    const float *weights_out, const float *bias_out,\n",
    "    float *grad_dW, float *grad_db,\n",
    "    float *grad_dW_out, float *grad_db_out,\n",
    "    int batch_size, int input_dim, int hidden_dim, int output_dim)\n",
    "{\n",
    "    int sample = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if(sample >= batch_size) return;\n",
    "\n",
    "    // Pointers for this sample.\n",
    "    const float *x = input + sample * input_dim;\n",
    "    const float *label = labels + sample * output_dim;\n",
    "    float *h = hidden + sample * hidden_dim;\n",
    "    float *out = output + sample * output_dim;\n",
    "\n",
    "    // Per-sample gradient pointers.\n",
    "    float *gradW = grad_dW + sample * hidden_dim * input_dim;\n",
    "    float *gradb = grad_db + sample * hidden_dim;\n",
    "    float *gradW_out = grad_dW_out + sample * output_dim * hidden_dim;\n",
    "    float *gradb_out = grad_db_out + sample * output_dim;\n",
    "\n",
    "    // Forward pass: hidden layer.\n",
    "    for (int i = 0; i < hidden_dim; i++) {\n",
    "        float sum = bias[i];\n",
    "        for (int j = 0; j < input_dim; j++) {\n",
    "            sum += weights[i * input_dim + j] * x[j];\n",
    "        }\n",
    "        h[i] = (sum > 0) ? sum : 0;  // ReLU.\n",
    "    }\n",
    "    // Output layer.\n",
    "    for (int i = 0; i < output_dim; i++) {\n",
    "        float sum = bias_out[i];\n",
    "        for (int j = 0; j < hidden_dim; j++) {\n",
    "            sum += weights_out[i * hidden_dim + j] * h[j];\n",
    "        }\n",
    "        out[i] = sum;\n",
    "    }\n",
    "\n",
    "    // Backpropagation.\n",
    "    // Compute delta for output: delta_out = out - label.\n",
    "    float delta_out[16]; // assume output_dim <= 16.\n",
    "    for (int i = 0; i < output_dim; i++) {\n",
    "        delta_out[i] = out[i] - label[i];\n",
    "        gradb_out[i] = delta_out[i];\n",
    "    }\n",
    "    // Gradients for output weights.\n",
    "    for (int i = 0; i < output_dim; i++) {\n",
    "        for (int j = 0; j < hidden_dim; j++) {\n",
    "            gradW_out[i * hidden_dim + j] = delta_out[i] * h[j];\n",
    "        }\n",
    "    }\n",
    "    // Backpropagate to hidden: delta_hidden = (W_out^T * delta_out) * (h>0)\n",
    "    float delta_hidden[128]; // assume hidden_dim <= 128.\n",
    "    for (int i = 0; i < hidden_dim; i++) {\n",
    "        float sum = 0;\n",
    "        for (int j = 0; j < output_dim; j++) {\n",
    "            sum += weights_out[j * hidden_dim + i] * delta_out[j];\n",
    "        }\n",
    "        delta_hidden[i] = (h[i] > 0) ? sum : 0;\n",
    "        gradb[i] = delta_hidden[i];\n",
    "    }\n",
    "    // Gradients for input-to-hidden weights.\n",
    "    for (int i = 0; i < hidden_dim; i++) {\n",
    "        for (int j = 0; j < input_dim; j++) {\n",
    "            gradW[i * input_dim + j] = delta_hidden[i] * x[j];\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "__global__ void updateWeights(\n",
    "    float *param, const float *grad_buffer,\n",
    "    int count, int batch_size, float learning_rate)\n",
    "{\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx >= count) return;\n",
    "    float sum = 0;\n",
    "    for (int i = 0; i < batch_size; i++) {\n",
    "        sum += grad_buffer[i * count + idx];\n",
    "    }\n",
    "    float grad_avg = sum / batch_size;\n",
    "    param[idx] -= learning_rate * grad_avg;\n",
    "}\n",
    "\n",
    "} // extern \"C\"\n",
    "\"\"\"\n",
    "\n",
    "# Compile the CUDA kernels.\n",
    "mod = SourceModule(cuda_kernel_code)\n",
    "trainStep = mod.get_function(\"trainStep\")\n",
    "updateWeights = mod.get_function(\"updateWeights\")\n",
    "\n",
    "# Initialize network parameters for PyCUDA.\n",
    "weights_cuda = (np.random.randn(hidden_dim, input_dim) * scale).astype(np.float32)\n",
    "bias_cuda    = (np.random.randn(hidden_dim) * scale).astype(np.float32)\n",
    "weights_out_cuda = (np.random.randn(output_dim, hidden_dim) * scale).astype(np.float32)\n",
    "bias_out_cuda    = (np.random.randn(output_dim) * scale).astype(np.float32)\n",
    "\n",
    "# Allocate GPU memory for parameters.\n",
    "weights_gpu = cuda.mem_alloc(weights_cuda.nbytes)\n",
    "bias_gpu    = cuda.mem_alloc(bias_cuda.nbytes)\n",
    "weights_out_gpu = cuda.mem_alloc(weights_out_cuda.nbytes)\n",
    "bias_out_gpu    = cuda.mem_alloc(bias_out_cuda.nbytes)\n",
    "cuda.memcpy_htod(weights_gpu, weights_cuda)\n",
    "cuda.memcpy_htod(bias_gpu, bias_cuda)\n",
    "cuda.memcpy_htod(weights_out_gpu, weights_out_cuda)\n",
    "cuda.memcpy_htod(bias_out_gpu, bias_out_cuda)\n",
    "\n",
    "# Allocate GPU memory for activations, outputs, inputs, labels, and gradients.\n",
    "hidden_gpu      = cuda.mem_alloc(batch_size * hidden_dim * np.float32().nbytes)\n",
    "output_gpu      = cuda.mem_alloc(batch_size * output_dim * np.float32().nbytes)\n",
    "input_gpu       = cuda.mem_alloc(batch_size * input_dim * np.float32().nbytes)\n",
    "labels_gpu      = cuda.mem_alloc(batch_size * output_dim * np.float32().nbytes)\n",
    "grad_dW_gpu     = cuda.mem_alloc(batch_size * hidden_dim * input_dim * np.float32().nbytes)\n",
    "grad_db_gpu     = cuda.mem_alloc(batch_size * hidden_dim * np.float32().nbytes)\n",
    "grad_dW_out_gpu = cuda.mem_alloc(batch_size * output_dim * hidden_dim * np.float32().nbytes)\n",
    "grad_db_out_gpu = cuda.mem_alloc(batch_size * output_dim * np.float32().nbytes)\n",
    "\n",
    "# Define grid and block dimensions.\n",
    "threads_per_block = 256\n",
    "grid_dim = ((batch_size + threads_per_block - 1) // threads_per_block, 1, 1)\n",
    "block_dim = (threads_per_block, 1, 1)\n",
    "\n",
    "print(\"\\nStarting PyCUDA Training (Two-Layer Network)...\")\n",
    "cuda_start = time.time()\n",
    "batch_count_cuda = 0\n",
    "accuracy_cuda = 0\n",
    "train_iter = iter(train_loader)\n",
    "while accuracy_cuda < 0.6 and batch_count_cuda < max_batches:\n",
    "    try:\n",
    "        images, labels = next(train_iter)\n",
    "    except StopIteration:\n",
    "        train_iter = iter(train_loader)\n",
    "        images, labels = next(train_iter)\n",
    "    x_batch = images.numpy()                     # shape: [batch_size, 784]\n",
    "    y_batch = one_hot(labels.numpy(), output_dim)  # one-hot encoding.\n",
    "    cuda.memcpy_htod(input_gpu, x_batch)\n",
    "    cuda.memcpy_htod(labels_gpu, y_batch)\n",
    "    # Launch forward/backprop kernel.\n",
    "    trainStep(\n",
    "        input_gpu, labels_gpu,\n",
    "        hidden_gpu, output_gpu,\n",
    "        weights_gpu, bias_gpu,\n",
    "        weights_out_gpu, bias_out_gpu,\n",
    "        grad_dW_gpu, grad_db_gpu,\n",
    "        grad_dW_out_gpu, grad_db_out_gpu,\n",
    "        np.int32(batch_size), np.int32(input_dim), np.int32(hidden_dim), np.int32(output_dim),\n",
    "        block=block_dim, grid=grid_dim\n",
    "    )\n",
    "    # Launch updateWeights for each parameter.\n",
    "    threads = 256\n",
    "\n",
    "    # Update weights (input-to-hidden).\n",
    "    count_w = np.int32(hidden_dim * input_dim)\n",
    "    grid_update = ((int(count_w) + threads - 1) // threads, 1, 1)\n",
    "    updateWeights(\n",
    "        weights_gpu, grad_dW_gpu,\n",
    "        count_w, np.int32(batch_size), np.float32(learning_rate),\n",
    "        block=(threads,1,1), grid=grid_update\n",
    "    )\n",
    "    # Update hidden biases.\n",
    "    count_b = np.int32(hidden_dim)\n",
    "    grid_update = ((int(count_b) + threads - 1) // threads, 1, 1)\n",
    "    updateWeights(\n",
    "        bias_gpu, grad_db_gpu,\n",
    "        count_b, np.int32(batch_size), np.float32(learning_rate),\n",
    "        block=(threads,1,1), grid=grid_update\n",
    "    )\n",
    "    # Update weights (hidden-to-output).\n",
    "    count_w_out = np.int32(output_dim * hidden_dim)\n",
    "    grid_update = ((int(count_w_out) + threads - 1) // threads, 1, 1)\n",
    "    updateWeights(\n",
    "        weights_out_gpu, grad_dW_out_gpu,\n",
    "        count_w_out, np.int32(batch_size), np.float32(learning_rate),\n",
    "        block=(threads,1,1), grid=grid_update\n",
    "    )\n",
    "    # Update output biases.\n",
    "    count_b_out = np.int32(output_dim)\n",
    "    grid_update = ((int(count_b_out) + threads - 1) // threads, 1, 1)\n",
    "    updateWeights(\n",
    "        bias_out_gpu, grad_db_out_gpu,\n",
    "        count_b_out, np.int32(batch_size), np.float32(learning_rate),\n",
    "        block=(threads,1,1), grid=grid_update\n",
    "    )\n",
    "    batch_count_cuda += 1\n",
    "\n",
    "    # Every 100 batches, evaluate test accuracy.\n",
    "    if batch_count_cuda % 100 == 0:\n",
    "        cuda.memcpy_dtoh(weights_cuda, weights_gpu)\n",
    "        cuda.memcpy_dtoh(bias_cuda, bias_gpu)\n",
    "        cuda.memcpy_dtoh(weights_out_cuda, weights_out_gpu)\n",
    "        cuda.memcpy_dtoh(bias_out_cuda, bias_out_gpu)\n",
    "        accuracy_cuda = evaluate_model(weights_cuda, bias_cuda, weights_out_cuda, bias_out_cuda)\n",
    "        print(f\"PyCUDA: Batch {batch_count_cuda}, Test Accuracy: {accuracy_cuda*100:.2f}%\")\n",
    "cuda_end = time.time()\n",
    "cuda_time = cuda_end - cuda_start\n",
    "print(f\"\\nPyCUDA Training Finished in {cuda_time:.4f} seconds over {batch_count_cuda} batches.\")\n",
    "print(f\"PyCUDA Test Accuracy: {accuracy_cuda*100:.2f}%\")\n",
    "\n",
    "###############################\n",
    "# 2. PyTorch Implementation\n",
    "###############################\n",
    "print(\"\\nStarting PyTorch Training...\")\n",
    "\n",
    "# Define a two-layer network in PyTorch with identical architecture.\n",
    "class TwoLayerNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim, bias=True)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim, bias=True)\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, input_dim)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Use MSELoss on one-hot targets to mimic the custom kernel.\n",
    "def to_one_hot(labels, num_classes=10):\n",
    "    one_hot = torch.zeros(labels.size(0), num_classes, device=labels.device)\n",
    "    one_hot.scatter_(1, labels.view(-1,1), 1.0)\n",
    "    return one_hot\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TwoLayerNet().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "torch_start = time.time()\n",
    "batch_count_torch = 0\n",
    "accuracy_torch = 0\n",
    "train_iter_torch = iter(train_loader)\n",
    "while accuracy_torch < 0.6 and batch_count_torch < max_batches:\n",
    "    try:\n",
    "        images, labels = next(train_iter_torch)\n",
    "    except StopIteration:\n",
    "        train_iter_torch = iter(train_loader)\n",
    "        images, labels = next(train_iter_torch)\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    # Convert labels to one-hot.\n",
    "    one_hot_labels = to_one_hot(labels, output_dim)\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(images)\n",
    "    loss = criterion(outputs, one_hot_labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    batch_count_torch += 1\n",
    "    if batch_count_torch % 100 == 0:\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(images)\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                total += labels.size(0)\n",
    "                correct += (preds == labels).sum().item()\n",
    "        accuracy_torch = correct / total\n",
    "        print(f\"PyTorch: Batch {batch_count_torch}, Test Accuracy: {accuracy_torch*100:.2f}%\")\n",
    "        model.train()\n",
    "torch_end = time.time()\n",
    "torch_time = torch_end - torch_start\n",
    "print(f\"\\nPyTorch Training Finished in {torch_time:.4f} seconds over {batch_count_torch} batches.\")\n",
    "print(f\"PyTorch Test Accuracy: {accuracy_torch*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Comparison with PyTorch\n",
    "\n",
    "The PyTorch implementation was set up to match the architecture and training procedure of our custom CUDA solution. In this setup:\n",
    "\n",
    "- **Architecture:**  \n",
    "  A two-layer network is used with one hidden layer of $128$ neurons (with ReLU activation) and an output layer of $10$ neurons. This exactly mirrors the network used in the PyCUDA implementation.\n",
    "\n",
    "- **Loss Function and Update Rule:**  \n",
    "  Both implementations use the Mean Squared Error (MSE) loss on one-hot encoded targets. The PyTorch model computes gradients on a per-mini-batch basis and updates parameters with a full batch gradient update, just as in the CUDA kernel that averages per-sample gradients over the mini-batch.\n",
    "\n",
    "- **Training Details:**  \n",
    "  Both models are trained on MNIST data with a batch size of $64$. The training loop in PyTorch updates the network after each mini-batch and evaluates test accuracy periodically (every 100 batches) until the test accuracy exceeds 60% or a maximum number of batches is reached.\n",
    "\n",
    "- **Results:**  \n",
    "  The results observed were as follows:\n",
    "  - **PyTorch:**  \n",
    "    - At Batch 100, Test Accuracy: 12.18%  \n",
    "    - At Batch 200, Test Accuracy: 13.38%  \n",
    "    - At Batch 300, Test Accuracy: 15.13%  \n",
    "    - At Batch 400, Test Accuracy: 17.71%  \n",
    "    - ...  \n",
    "    - At Batch 4200, Test Accuracy: 60.33%  \n",
    "    - Total Training Time: 67.6265 seconds over 4200 batches  \n",
    "  - **PyCUDA:**  \n",
    "    - At Batch 100, Test Accuracy: 13.01%  \n",
    "    - At Batch 200, Test Accuracy: 14.63%  \n",
    "    - At Batch 300, Test Accuracy: 17.43%  \n",
    "    - At Batch 400, Test Accuracy: 22.13%  \n",
    "    - ...  \n",
    "    - At Batch 1200, Test Accuracy: 61.64%  \n",
    "    - Total Training Time: 33.4456 seconds over 1200 batches  \n",
    "\n",
    "**Analysis:**  \n",
    "Both methods are trained on the same amount of data and use an identical network architecture and loss function. The PyTorch model, while being easier to code and more robust, took more batches and nearly twice the time to reach about 60% accuracy compared to the custom PyCUDA solution. This highlights that, with careful low-level optimization, custom CUDA code can offer performance benefits; however, such implementations require a more complex development process compared to high-level frameworks like PyTorch.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
