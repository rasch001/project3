{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "forward_pass = r\"\"\"\n",
    ".version 7.0\n",
    ".target sm_35\n",
    ".address_size 64\n",
    "\n",
    ".visible .entry neuralNet(\n",
    "    .param .u64 input_ptr,       // Pointer to input vector (e.g. 784 floats)\n",
    "    .param .u64 weights1_ptr,    // Weights for Layer 1 (hidden1_dim x input_dim)\n",
    "    .param .u64 bias1_ptr,       // Bias for Layer 1 (hidden1_dim)\n",
    "    .param .u64 hidden1_ptr,     // Output buffer for Layer 1 (hidden1_dim)\n",
    "    .param .u64 weights2_ptr,    // Weights for Layer 2 (hidden2_dim x hidden1_dim)\n",
    "    .param .u64 bias2_ptr,       // Bias for Layer 2 (hidden2_dim)\n",
    "    .param .u64 hidden2_ptr,     // Output buffer for Layer 2 (hidden2_dim)\n",
    "    .param .u64 weights3_ptr,    // Weights for Layer 3 (output_dim x hidden2_dim)\n",
    "    .param .u64 bias3_ptr,       // Bias for Layer 3 (output_dim)\n",
    "    .param .u64 output_ptr,      // Output buffer for final result (output_dim, e.g. 10)\n",
    "    .param .u32 input_dim,       // Dimension of input vector (e.g. 784)\n",
    "    .param .u32 hidden1_dim,     // Number of neurons in Layer 1\n",
    "    .param .u32 hidden2_dim,     // Number of neurons in Layer 2\n",
    "    .param .u32 output_dim       // Dimension of output vector (10)\n",
    ")\n",
    "{\n",
    "    // Declare registers.\n",
    "    .reg .u64  in_base, w1_base, b1_base, h1_base, w2_base, b2_base, h2_base, w3_base, b3_base, out_base;\n",
    "    .reg .u32  inputDim, hidden1Dim, hidden2Dim, outputDim;\n",
    "    .reg .s32  i, j, idx, idx_byte;\n",
    "    .reg .u64  addr_in, addr_w, addr_bias, addr_h1, addr_hidden, addr_h2, addr_out, byte_offset;\n",
    "    .reg .f32  acc, temp, in_val, wt, bias;\n",
    "    .reg .pred p_exit;\n",
    "\n",
    "    // Load kernel parameters.\n",
    "    ld.param.u64   in_base, [input_ptr];\n",
    "    ld.param.u64   w1_base, [weights1_ptr];\n",
    "    ld.param.u64   b1_base, [bias1_ptr];\n",
    "    ld.param.u64   h1_base, [hidden1_ptr];\n",
    "    ld.param.u64   w2_base, [weights2_ptr];\n",
    "    ld.param.u64   b2_base, [bias2_ptr];\n",
    "    ld.param.u64   h2_base, [hidden2_ptr];\n",
    "    ld.param.u64   w3_base, [weights3_ptr];\n",
    "    ld.param.u64   b3_base, [bias3_ptr];\n",
    "    ld.param.u64   out_base, [output_ptr];\n",
    "    ld.param.u32   inputDim, [input_dim];\n",
    "    ld.param.u32   hidden1Dim, [hidden1_dim];\n",
    "    ld.param.u32   hidden2Dim, [hidden2_dim];\n",
    "    ld.param.u32   outputDim, [output_dim];\n",
    "\n",
    "    // ----------------------------------\n",
    "    // Layer 1: Input -> Hidden1 (ReLU)\n",
    "    // ----------------------------------\n",
    "    mov.s32   j, 0;\n",
    "layer1_loop:\n",
    "    setp.ge.s32 p_exit, j, hidden1Dim;\n",
    "    @p_exit bra layer1_end;\n",
    "\n",
    "    mov.f32   acc, 0f00000000;  // Initialize accumulator for neuron j\n",
    "    mov.s32   i, 0;\n",
    "layer1_inner_loop:\n",
    "    setp.ge.s32 p_exit, i, inputDim;\n",
    "    @p_exit bra layer1_after_inner;\n",
    "\n",
    "    // Compute weight index: idx = j * inputDim + i\n",
    "    mul.lo.s32 idx, j, inputDim;\n",
    "    add.s32    idx, idx, i;\n",
    "    mul.lo.s32 idx_byte, idx, 4;        // Convert index to byte offset.\n",
    "    cvt.u64.s32 byte_offset, idx_byte;\n",
    "    add.u64    addr_w, w1_base, byte_offset;\n",
    "    ld.global.f32 wt, [addr_w];\n",
    "\n",
    "    // Load input value: address = in_base + (i*4)\n",
    "    mul.lo.s32 idx_byte, i, 4;\n",
    "    cvt.u64.s32 byte_offset, idx_byte;\n",
    "    add.u64    addr_in, in_base, byte_offset;\n",
    "    ld.global.f32 in_val, [addr_in];\n",
    "\n",
    "    // Multiply and accumulate.\n",
    "    mul.f32    temp, wt, in_val;\n",
    "    add.f32    acc, acc, temp;\n",
    "\n",
    "    add.s32    i, i, 1;\n",
    "    bra        layer1_inner_loop;\n",
    "layer1_after_inner:\n",
    "    // Add bias: bias for neuron j is at b1_base + (j*4)\n",
    "    mul.lo.s32 idx_byte, j, 4;\n",
    "    cvt.u64.s32 byte_offset, idx_byte;\n",
    "    add.u64    addr_bias, b1_base, byte_offset;\n",
    "    ld.global.f32 bias, [addr_bias];\n",
    "    add.f32    acc, acc, bias;\n",
    "\n",
    "    // Apply ReLU activation.\n",
    "    max.f32    acc, acc, 0f00000000;\n",
    "\n",
    "    // Store result in hidden layer 1: h1_base + (j*4)\n",
    "    mul.lo.s32 idx_byte, j, 4;\n",
    "    cvt.u64.s32 byte_offset, idx_byte;\n",
    "    add.u64    addr_h1, h1_base, byte_offset;\n",
    "    st.global.f32 [addr_h1], acc;\n",
    "\n",
    "    add.s32    j, j, 1;\n",
    "    bra        layer1_loop;\n",
    "layer1_end:\n",
    "\n",
    "    // ----------------------------------\n",
    "    // Layer 2: Hidden1 -> Hidden2 (ReLU)\n",
    "    // ----------------------------------\n",
    "    mov.s32   j, 0;\n",
    "layer2_loop:\n",
    "    setp.ge.s32 p_exit, j, hidden2Dim;\n",
    "    @p_exit bra layer2_end;\n",
    "\n",
    "    mov.f32   acc, 0f00000000;  // Accumulator for neuron j in layer 2\n",
    "    mov.s32   i, 0;\n",
    "layer2_inner_loop:\n",
    "    setp.ge.s32 p_exit, i, hidden1Dim;\n",
    "    @p_exit bra layer2_after_inner;\n",
    "\n",
    "    // Weight index: idx = j * hidden1Dim + i\n",
    "    mul.lo.s32 idx, j, hidden1Dim;\n",
    "    add.s32    idx, idx, i;\n",
    "    mul.lo.s32 idx_byte, idx, 4;\n",
    "    cvt.u64.s32 byte_offset, idx_byte;\n",
    "    add.u64    addr_w, w2_base, byte_offset;\n",
    "    ld.global.f32 wt, [addr_w];\n",
    "\n",
    "    // Load hidden1 value: h1_base + (i*4)\n",
    "    mul.lo.s32 idx_byte, i, 4;\n",
    "    cvt.u64.s32 byte_offset, idx_byte;\n",
    "    add.u64    addr_hidden, h1_base, byte_offset;\n",
    "    ld.global.f32 in_val, [addr_hidden];\n",
    "\n",
    "    mul.f32    temp, wt, in_val;\n",
    "    add.f32    acc, acc, temp;\n",
    "\n",
    "    add.s32    i, i, 1;\n",
    "    bra        layer2_inner_loop;\n",
    "layer2_after_inner:\n",
    "    // Add bias for layer 2 neuron j.\n",
    "    mul.lo.s32 idx_byte, j, 4;\n",
    "    cvt.u64.s32 byte_offset, idx_byte;\n",
    "    add.u64    addr_bias, b2_base, byte_offset;\n",
    "    ld.global.f32 bias, [addr_bias];\n",
    "    add.f32    acc, acc, bias;\n",
    "\n",
    "    // Apply ReLU activation.\n",
    "    max.f32    acc, acc, 0f00000000;\n",
    "\n",
    "    // Store in hidden layer 2: h2_base + (j*4)\n",
    "    mul.lo.s32 idx_byte, j, 4;\n",
    "    cvt.u64.s32 byte_offset, idx_byte;\n",
    "    add.u64    addr_h2, h2_base, byte_offset;\n",
    "    st.global.f32 [addr_h2], acc;\n",
    "\n",
    "    add.s32    j, j, 1;\n",
    "    bra        layer2_loop;\n",
    "layer2_end:\n",
    "\n",
    "    // ----------------------------------\n",
    "    // Layer 3: Hidden2 -> Output (linear)\n",
    "    // ----------------------------------\n",
    "    mov.s32   j, 0;\n",
    "layer3_loop:\n",
    "    setp.ge.s32 p_exit, j, outputDim;\n",
    "    @p_exit bra layer3_end;\n",
    "\n",
    "    mov.f32   acc, 0f00000000;  // Accumulator for output neuron j\n",
    "    mov.s32   i, 0;\n",
    "layer3_inner_loop:\n",
    "    setp.ge.s32 p_exit, i, hidden2Dim;\n",
    "    @p_exit bra layer3_after_inner;\n",
    "\n",
    "    // Weight index: idx = j * hidden2Dim + i\n",
    "    mul.lo.s32 idx, j, hidden2Dim;\n",
    "    add.s32    idx, idx, i;\n",
    "    mul.lo.s32 idx_byte, idx, 4;\n",
    "    cvt.u64.s32 byte_offset, idx_byte;\n",
    "    add.u64    addr_w, w3_base, byte_offset;\n",
    "    ld.global.f32 wt, [addr_w];\n",
    "\n",
    "    // Load hidden2 value: h2_base + (i*4)\n",
    "    mul.lo.s32 idx_byte, i, 4;\n",
    "    cvt.u64.s32 byte_offset, idx_byte;\n",
    "    add.u64    addr_hidden, h2_base, byte_offset;\n",
    "    ld.global.f32 in_val, [addr_hidden];\n",
    "\n",
    "    mul.f32    temp, wt, in_val;\n",
    "    add.f32    acc, acc, temp;\n",
    "\n",
    "    add.s32    i, i, 1;\n",
    "    bra        layer3_inner_loop;\n",
    "layer3_after_inner:\n",
    "    // Add bias for output neuron j.\n",
    "    mul.lo.s32 idx_byte, j, 4;\n",
    "    cvt.u64.s32 byte_offset, idx_byte;\n",
    "    add.u64    addr_bias, b3_base, byte_offset;\n",
    "    ld.global.f32 bias, [addr_bias];\n",
    "    add.f32    acc, acc, bias;\n",
    "\n",
    "    // Write final output: out_base + (j*4)\n",
    "    mul.lo.s32 idx_byte, j, 4;\n",
    "    cvt.u64.s32 byte_offset, idx_byte;\n",
    "    add.u64    addr_out, out_base, byte_offset;\n",
    "    st.global.f32 [addr_out], acc;\n",
    "\n",
    "    add.s32    j, j, 1;\n",
    "    bra        layer3_loop;\n",
    "layer3_end:\n",
    "\n",
    "    ret;\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Architecture\n",
    "\n",
    "Let the input be:\n",
    "$$\n",
    "\\mathbf{x} \\in \\mathbb{R}^{d} \\quad \\text{with } d=784 \\quad (\\text{MNIST image flattened})\n",
    "$$\n",
    "\n",
    "The network has three layers defined as follows:\n",
    "\n",
    "### Layer 1 (Input to Hidden1)\n",
    "\n",
    "- **Weights:**  \n",
    "  $$\n",
    "  W^{(1)} \\in \\mathbb{R}^{n_1 \\times d}\n",
    "  $$\n",
    "  Each element is \\( W^{(1)}_{ij} \\) with \\( i = 1, \\dots, n_1 \\) and \\( j = 1, \\dots, d \\).\n",
    "\n",
    "- **Biases:**  \n",
    "  $$\n",
    "  \\mathbf{b}^{(1)} \\in \\mathbb{R}^{n_1}\n",
    "  $$\n",
    "  with components \\( b^{(1)}_{i} \\).\n",
    "\n",
    "- **Pre-activation:**  \n",
    "  $$\n",
    "  \\mathbf{z}^{(1)} = W^{(1)} \\mathbf{x} + \\mathbf{b}^{(1)}\n",
    "  $$\n",
    "\n",
    "- **Activation (ReLU):**  \n",
    "  $$\n",
    "  \\mathbf{a}^{(1)} = \\sigma(\\mathbf{z}^{(1)}) \\quad \\text{with } \\sigma(z) = \\max(0, z)\n",
    "  $$\n",
    "\n",
    "### Layer 2 (Hidden1 to Hidden2)\n",
    "\n",
    "- **Weights:**  \n",
    "  $$\n",
    "  W^{(2)} \\in \\mathbb{R}^{n_2 \\times n_1}\n",
    "  $$\n",
    "  Each element is \\( W^{(2)}_{ij} \\) for \\( i = 1, \\dots, n_2 \\) and \\( j = 1, \\dots, n_1 \\).\n",
    "\n",
    "- **Biases:**  \n",
    "  $$\n",
    "  \\mathbf{b}^{(2)} \\in \\mathbb{R}^{n_2}\n",
    "  $$\n",
    "  with components \\( b^{(2)}_{i} \\).\n",
    "\n",
    "- **Pre-activation:**  \n",
    "  $$\n",
    "  \\mathbf{z}^{(2)} = W^{(2)} \\mathbf{a}^{(1)} + \\mathbf{b}^{(2)}\n",
    "  $$\n",
    "\n",
    "- **Activation (ReLU):**  \n",
    "  $$\n",
    "  \\mathbf{a}^{(2)} = \\sigma(\\mathbf{z}^{(2)}) \\quad \\text{with } \\sigma(z) = \\max(0, z)\n",
    "  $$\n",
    "\n",
    "### Layer 3 (Hidden2 to Output)\n",
    "\n",
    "- **Weights:**  \n",
    "  $$\n",
    "  W^{(3)} \\in \\mathbb{R}^{10 \\times n_2}\n",
    "  $$\n",
    "  Each element is \\( W^{(3)}_{ij} \\) for \\( i = 1, \\dots, 10 \\) and \\( j = 1, \\dots, n_2 \\).\n",
    "\n",
    "- **Biases:**  \n",
    "  $$\n",
    "  \\mathbf{b}^{(3)} \\in \\mathbb{R}^{10}\n",
    "  $$\n",
    "  with components \\( b^{(3)}_{i} \\).\n",
    "\n",
    "- **Pre-activation (Logits):**  \n",
    "  $$\n",
    "  \\mathbf{z}^{(3)} = W^{(3)} \\mathbf{a}^{(2)} + \\mathbf{b}^{(3)}\n",
    "  $$\n",
    "\n",
    "- **Output:**  \n",
    "  Often, a softmax is applied to \\(\\mathbf{z}^{(3)}\\) to obtain probabilities:\n",
    "  $$\n",
    "  \\hat{\\mathbf{y}} = \\operatorname{softmax}(\\mathbf{z}^{(3)}) \\quad \\text{where } \\hat{y}_i = \\frac{e^{z^{(3)}_i}}{\\sum_{k=1}^{10} e^{z^{(3)}_k}}\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "## Loss Function\n",
    "\n",
    "Assume we use the cross-entropy loss for classification. For a single training example with true label vector \\(\\mathbf{y}\\) (one-hot encoded), the loss is:\n",
    "$$\n",
    "L = -\\sum_{i=1}^{10} y_i \\log(\\hat{y}_i)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Backpropagation\n",
    "\n",
    "Let the derivative of the loss with respect to any variable \\( u \\) be denoted by \\(\\frac{\\partial L}{\\partial u}\\). We define the error terms (or deltas) for each layer.\n",
    "\n",
    "### Output Layer (Layer 3)\n",
    "\n",
    "For the output layer using softmax and cross-entropy, the error is:\n",
    "$$\n",
    "\\delta^{(3)} = \\hat{\\mathbf{y}} - \\mathbf{y}\n",
    "$$\n",
    "\n",
    "The gradients for layer 3 are:\n",
    "\n",
    "- **Weights:**\n",
    "  $$\n",
    "  \\frac{\\partial L}{\\partial W^{(3)}} = \\delta^{(3)} \\, (\\mathbf{a}^{(2)})^T\n",
    "  $$\n",
    "\n",
    "- **Biases:**\n",
    "  $$\n",
    "  \\frac{\\partial L}{\\partial \\mathbf{b}^{(3)}} = \\delta^{(3)}\n",
    "  $$\n",
    "\n",
    "### Hidden Layer 2 (Layer 2)\n",
    "\n",
    "The error for layer 2 is computed by backpropagating the error from layer 3 and applying the derivative of the ReLU activation. Recall:\n",
    "$$\n",
    "\\sigma'(z) = \\begin{cases} \n",
    "1 & \\text{if } z > 0, \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Thus,\n",
    "$$\n",
    "\\delta^{(2)} = \\left( W^{(3)} \\right)^T \\delta^{(3)} \\circ \\sigma'(\\mathbf{z}^{(2)})\n",
    "$$\n",
    "where “\\(\\circ\\)” denotes element-wise multiplication.\n",
    "\n",
    "The gradients for layer 2 are:\n",
    "\n",
    "- **Weights:**\n",
    "  $$\n",
    "  \\frac{\\partial L}{\\partial W^{(2)}} = \\delta^{(2)} \\, (\\mathbf{a}^{(1)})^T\n",
    "  $$\n",
    "\n",
    "- **Biases:**\n",
    "  $$\n",
    "  \\frac{\\partial L}{\\partial \\mathbf{b}^{(2)}} = \\delta^{(2)}\n",
    "  $$\n",
    "\n",
    "### Hidden Layer 1 (Layer 1)\n",
    "\n",
    "Similarly, backpropagate the error to the first hidden layer:\n",
    "$$\n",
    "\\delta^{(1)} = \\left( W^{(2)} \\right)^T \\delta^{(2)} \\circ \\sigma'(\\mathbf{z}^{(1)})\n",
    "$$\n",
    "\n",
    "The gradients for layer 1 are:\n",
    "\n",
    "- **Weights:**\n",
    "  $$\n",
    "  \\frac{\\partial L}{\\partial W^{(1)}} = \\delta^{(1)} \\, (\\mathbf{x})^T\n",
    "  $$\n",
    "\n",
    "- **Biases:**\n",
    "  $$\n",
    "  \\frac{\\partial L}{\\partial \\mathbf{b}^{(1)}} = \\delta^{(1)}\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Forward Pass\n",
    "\n",
    "1. **Layer 1:**\n",
    "   $$\n",
    "   \\mathbf{z}^{(1)} = W^{(1)} \\mathbf{x} + \\mathbf{b}^{(1)}, \\quad \\mathbf{a}^{(1)} = \\max(0, \\mathbf{z}^{(1)})\n",
    "   $$\n",
    "\n",
    "2. **Layer 2:**\n",
    "   $$\n",
    "   \\mathbf{z}^{(2)} = W^{(2)} \\mathbf{a}^{(1)} + \\mathbf{b}^{(2)}, \\quad \\mathbf{a}^{(2)} = \\max(0, \\mathbf{z}^{(2)})\n",
    "   $$\n",
    "\n",
    "3. **Layer 3:**\n",
    "   $$\n",
    "   \\mathbf{z}^{(3)} = W^{(3)} \\mathbf{a}^{(2)} + \\mathbf{b}^{(3)}, \\quad \\hat{\\mathbf{y}} = \\operatorname{softmax}(\\mathbf{z}^{(3)})\n",
    "   $$\n",
    "\n",
    "### Backward Pass\n",
    "\n",
    "1. **Output Layer (Layer 3):**\n",
    "   $$\n",
    "   \\delta^{(3)} = \\hat{\\mathbf{y}} - \\mathbf{y}\n",
    "   $$\n",
    "   $$\n",
    "   \\frac{\\partial L}{\\partial W^{(3)}} = \\delta^{(3)} (\\mathbf{a}^{(2)})^T, \\quad \\frac{\\partial L}{\\partial \\mathbf{b}^{(3)}} = \\delta^{(3)}\n",
    "   $$\n",
    "\n",
    "2. **Hidden Layer 2 (Layer 2):**\n",
    "   $$\n",
    "   \\delta^{(2)} = \\left( W^{(3)} \\right)^T \\delta^{(3)} \\circ 1_{\\{\\mathbf{z}^{(2)} > 0\\}}\n",
    "   $$\n",
    "   $$\n",
    "   \\frac{\\partial L}{\\partial W^{(2)}} = \\delta^{(2)} (\\mathbf{a}^{(1)})^T, \\quad \\frac{\\partial L}{\\partial \\mathbf{b}^{(2)}} = \\delta^{(2)}\n",
    "   $$\n",
    "\n",
    "3. **Hidden Layer 1 (Layer 1):**\n",
    "   $$\n",
    "   \\delta^{(1)} = \\left( W^{(2)} \\right)^T \\delta^{(2)} \\circ 1_{\\{\\mathbf{z}^{(1)} > 0\\}}\n",
    "   $$\n",
    "   $$\n",
    "   \\frac{\\partial L}{\\partial W^{(1)}} = \\delta^{(1)} (\\mathbf{x})^T, \\quad \\frac{\\partial L}{\\partial \\mathbf{b}^{(1)}} = \\delta^{(1)}\n",
    "   $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
